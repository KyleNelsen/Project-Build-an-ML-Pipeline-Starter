diff --git a/main.py b/main.py
index 9abc5bb..76c3b0f 100644
--- a/main.py
+++ b/main.py
@@ -1,95 +1,127 @@
-import json
-
-import mlflow
-import tempfile
-import os
-import wandb
-import hydra
-from omegaconf import DictConfig
-
-_steps = [
-    "download",
-    "basic_cleaning",
-    "data_check",
-    "data_split",
-    "train_random_forest",
-    # NOTE: We do not include this in the steps so it is not run by mistake.
-    # You first need to promote a model export to "prod" before you can run this,
-    # then you need to run this step explicitly
-#    "test_regression_model"
-]
-
-
-# This automatically reads in the configuration
-@hydra.main(config_name='config')
-def go(config: DictConfig):
-
-    # Setup the wandb experiment. All runs will be grouped under this name
-    os.environ["WANDB_PROJECT"] = config["main"]["project_name"]
-    os.environ["WANDB_RUN_GROUP"] = config["main"]["experiment_name"]
-
-    # Steps to execute
-    steps_par = config['main']['steps']
-    active_steps = steps_par.split(",") if steps_par != "all" else _steps
-
-    # Move to a temporary directory
-    with tempfile.TemporaryDirectory() as tmp_dir:
-
-        if "download" in active_steps:
-            # Download file and load in W&B
-            _ = mlflow.run(
-                f"{config['main']['components_repository']}/get_data",
-                "main",
-                parameters={
-                    "sample": config["etl"]["sample"],
-                    "artifact_name": "sample.csv",
-                    "artifact_type": "raw_data",
-                    "artifact_description": "Raw file as downloaded"
-                },
-            )
-
-        if "basic_cleaning" in active_steps:
-            ##################
-            # Implement here #
-            ##################
-            pass
-
-        if "data_check" in active_steps:
-            ##################
-            # Implement here #
-            ##################
-            pass
-
-        if "data_split" in active_steps:
-            ##################
-            # Implement here #
-            ##################
-            pass
-
-        if "train_random_forest" in active_steps:
-
-            # NOTE: we need to serialize the random forest configuration into JSON
-            rf_config = os.path.abspath("rf_config.json")
-            with open(rf_config, "w+") as fp:
-                json.dump(dict(config["modeling"]["random_forest"].items()), fp)  # DO NOT TOUCH
-
-            # NOTE: use the rf_config we just created as the rf_config parameter for the train_random_forest
-            # step
-
-            ##################
-            # Implement here #
-            ##################
-
-            pass
-
-        if "test_regression_model" in active_steps:
-
-            ##################
-            # Implement here #
-            ##################
-
-            pass
-
-
-if __name__ == "__main__":
-    go()
+import json
+
+import mlflow
+import tempfile
+import os
+import wandb
+import hydra
+from omegaconf import DictConfig
+
+_steps = [
+    "download",
+    "basic_cleaning",
+    "data_check",
+    "data_split",
+    "train_random_forest",
+    # NOTE: We do not include this in the steps so it is not run by mistake.
+    # You first need to promote a model export to "prod" before you can run this,
+    # then you need to run this step explicitly
+#    "test_regression_model"
+]
+
+
+# This automatically reads in the configuration
+@hydra.main(config_name='config')
+def go(config: DictConfig):
+
+    # Setup the wandb experiment. All runs will be grouped under this name
+    os.environ["WANDB_PROJECT"] = config["main"]["project_name"]
+    os.environ["WANDB_RUN_GROUP"] = config["main"]["experiment_name"]
+
+    # Steps to execute
+    steps_par = config['main']['steps']
+    active_steps = steps_par.split(",") if steps_par != "all" else _steps
+
+    # Move to a temporary directory
+    with tempfile.TemporaryDirectory() as tmp_dir:
+
+        if "download" in active_steps:
+            # Download file and load in W&B
+            _ = mlflow.run(
+                f"{config['main']['components_repository']}/get_data",
+                "main",
+                parameters={
+                    "sample": config["etl"]["sample"],
+                    "artifact_name": "sample.csv",
+                    "artifact_type": "raw_data",
+                    "artifact_description": "Raw file as downloaded"
+                },
+            )
+
+        if "basic_cleaning" in active_steps:
+            _ = mlflow.run(
+                os.path.join(hydra.utils.get_original_cwd(), "src", "basic_cleaning"),
+                "main",
+                parameters={
+                    "input_artifact": "sample.csv:latest",
+                    "output_artifact": "clean_sample.csv",
+                    "output_type": "clean_sample",
+                    "output_description": "Data with outliers and null values removed",
+                    "min_price": config['etl']['min_price'],
+                    "max_price": config['etl']['max_price']
+                },
+           )
+
+        if "data_check" in active_steps:
+            _ = mlflow.run(
+                os.path.join(hydra.utils.get_original_cwd(), "src", "data_check"),
+                "main",
+                parameters={
+                    "csv": "clean_sample.csv:latest",
+                    "ref": "clean_sample.csv:reference",
+                    "kl_threshold": config["data_check"]["kl_threshold"],
+                    "min_price": config['etl']['min_price'],
+                    "max_price": config['etl']['max_price']
+                },
+            )
+
+        if "data_split" in active_steps:
+            _ = mlflow.run(
+                f"{config['main']['components_repository']}/train_val_test_split",
+                "main",
+                parameters={
+                    "input": "clean_sample.csv:latest",
+                    "test_size": config["modeling"]["test_size"],
+                    "random_seed": config["modeling"]["random_seed"],
+                    "stratify_by": config["modeling"]["stratify_by"]
+                },
+            )
+
+        if "train_random_forest" in active_steps:
+
+            # NOTE: we need to serialize the random forest configuration into JSON
+            rf_config = os.path.abspath("rf_config.json")
+            with open(rf_config, "w+") as fp:
+                json.dump(dict(config["modeling"]["random_forest"].items()), fp)  # DO NOT TOUCH
+
+            # NOTE: use the rf_config we just created as the rf_config parameter for the train_random_forest
+            # step
+
+            _ =mlflow.run(
+               os.path.join(root,"src","train_random_forest"),
+                "main",
+                parameters={
+                    "trainval_artifact": "trainval_data.csv:latest",
+                    "val_size": str(config['modeling']['val_size']),
+                    "random_seed": str(config['modeling']['random_seed']),
+                    "stratify_by": config['modeling']['stratify_by'],
+                    "rf_config": rf_config,
+                    "max_tfidf_features": str(config['modeling']['max_tfidf_features']),
+                    "output_artifact": "random_forest_export"
+                },
+            )
+
+        if "test_regression_model" in active_steps:
+
+            _ =mlflow.run(
+               os.path.join(root, "components", "test_regression_model"),
+                "main",
+                parameters={
+                    "mlflow_model": "random_forest_export:prod",
+                    "test_dataset": "test_data.csv:latest"
+                },
+            )
+
+
+if __name__ == "__main__":
+    go()
diff --git a/src/basic_cleaning/MLproject b/src/basic_cleaning/MLproject
index 7bd69e1..70eadc2 100644
--- a/src/basic_cleaning/MLproject
+++ b/src/basic_cleaning/MLproject
@@ -1,34 +1,34 @@
-name: basic_cleaning
-conda_env: conda.yml
-
-entry_points:
-  main:
-    parameters:
-
-      input_artifact:
-        description: Ininital artifact to be cleaned
-        type: string
-
-      output_artifact:
-        description: Output artifact for cleaned data
-        type: string
-
-      output_type:
-        description: Type of the output dataset
-        type: string
-
-      output_description:
-        description: Description of the output dataset
-        type: string
-
-      min_price:
-        description: Minumim house price to be considered
-        type: float
-
-      max_price:
-        description: Maximum house price to be considered
-        type: float
-
-
-    command: >-
-        python run.py  --input_artifact {input_artifact}  --output_artifact {output_artifact}  --output_type {output_type}  --output_description {output_description}  --min_price {min_price}  --max_price {max_price} 
+name: basic_cleaning
+conda_env: conda.yml
+
+entry_points:
+  main:
+    parameters:
+
+      input_artifact:
+        description: Ininital artifact to be cleaned
+        type: string
+
+      output_artifact:
+        description: Output artifact for cleaned data
+        type: string
+
+      output_type:
+        description: Type of the output dataset
+        type: string
+
+      output_description:
+        description: Description of the output dataset
+        type: string
+
+      min_price:
+        description: Minumim house price to be considered
+        type: float
+
+      max_price:
+        description: Maximum house price to be considered
+        type: float
+
+
+    command: >-
+        python run.py  --input_artifact {input_artifact}  --output_artifact {output_artifact}  --output_type {output_type}  --output_description {output_description}  --min_price {min_price}  --max_price {max_price} 
diff --git a/src/basic_cleaning/conda.yml b/src/basic_cleaning/conda.yml
index f35e98c..44ac9a7 100644
--- a/src/basic_cleaning/conda.yml
+++ b/src/basic_cleaning/conda.yml
@@ -1,9 +1,9 @@
-name: basic_cleaning
-channels:
-  - conda-forge
-  - defaults
-dependencies:
-  - pip=20.3.3
-  - pandas=1.2.3
-  - pip:
-      - wandb==0.13.9
+name: basic_cleaning
+channels:
+  - conda-forge
+  - defaults
+dependencies:
+  - pip=20.3.3
+  - pandas=1.2.3
+  - pip:
+      - wandb==0.13.9
diff --git a/src/basic_cleaning/run.py b/src/basic_cleaning/run.py
index b496452..4aacf90 100644
--- a/src/basic_cleaning/run.py
+++ b/src/basic_cleaning/run.py
@@ -1,100 +1,101 @@
-#!/usr/bin/env python
-"""
-Download from W&B the raw dataset and apply some basic data cleaning, exporting the result to a new artifact
-"""
-import argparse
-import logging
-import wandb
-import pandas as pd
-
-# DO NOT MODIFY
-logging.basicConfig(level=logging.INFO, format="%(asctime)-15s %(message)s")
-logger = logging.getLogger()
-
-# DO NOT MODIFY
-def go(args):
-    
-    logger.info('Starting wandb run.')
-    run = wandb.init(
-        project = 'nyc_airbnb',
-        group = 'basic_cleaning',
-        job_type="basic_cleaning" 
-    )
-    run.config.update(args)
-    # Download input artifact. This will also log that this script is using this
-    # particular version of the artifact
-    logger.info('Fetching raw dataset.')
-    local_path = wandb.use_artifact('sample.csv:latest').file()
-    df = pd.read_csv(local_path)
-    
-    # EDA with arguments passed into the step
-    logger.info('Cleaning data.')
-    idx = df['price'].between(float(args.min_price), float(args.max_price))
-    df = df[idx].copy()
-    df['last_review'] = pd.to_datetime(df['last_review'])
-    # TODO: add code to fix the issue happened when testing the model
-    
-
-    # Save the cleaned data
-    logger.info('Saving and exporting cleaned data.')
-    df.to_csv('clean_sample.csv', index=False)
-    artifact = wandb.Artifact(
-        args.output_artifact,
-        type = args.output_type,
-        description = args.output_description
-    )
-    artifact.add_file('clean_sample.csv')
-    run.log_artifact(artifact)
-    
-# TODO: In the code below, fill in the data type for each argumemt. The data type should be str, float or int. 
-# TODO: In the code below, fill in a description for each argument. The description should be a string.
-if __name__ == "__main__":
-
-    parser = argparse.ArgumentParser(description="A very basic data cleaning")
-  
-    parser.add_argument(
-        "--input_artifact", 
-        type = ## INSERT TYPE HERE: str, float or int,
-        help = ## INSERT DESCRIPTION HERE,
-        required = True
-    )
-
-    parser.add_argument(
-        "--output_artifact", 
-        type = ## INSERT TYPE HERE: str, float or int,
-        help = ## INSERT DESCRIPTION HERE,
-        required = True
-    )
-
-    parser.add_argument(
-        "--output_type", 
-        type = ## INSERT TYPE HERE: str, float or int,
-        help = ## INSERT DESCRIPTION HERE,
-        required = True
-    )
-
-    parser.add_argument(
-        "--output_description", 
-        type = ## INSERT TYPE HERE: str, float or int,
-        help = ## INSERT DESCRIPTION HERE,
-        required = True
-    )
-
-    parser.add_argument(
-        "--min_price", 
-        type = ## INSERT TYPE HERE: str, float or int,
-        help = ## INSERT DESCRIPTION HERE,
-        required = True
-    )
-
-    parser.add_argument(
-        "--max_price",
-        type = ## INSERT TYPE HERE: str, float or int,
-        help = ## INSERT DESCRIPTION HERE,
-        required = True
-    )
-
-
-    args = parser.parse_args()
-
-    go(args)
+#!/usr/bin/env python
+"""
+Download from W&B the raw dataset and apply some basic data cleaning, exporting the result to a new artifact
+"""
+import argparse
+import logging
+import wandb
+import pandas as pd
+
+# DO NOT MODIFY
+logging.basicConfig(level=logging.INFO, format="%(asctime)-15s %(message)s")
+logger = logging.getLogger()
+
+# DO NOT MODIFY
+def go(args):
+    
+    logger.info('Starting wandb run.')
+    run = wandb.init(
+        project = 'nyc_airbnb',
+        group = 'basic_cleaning',
+        job_type="basic_cleaning" 
+    )
+    run.config.update(args)
+    # Download input artifact. This will also log that this script is using this
+    # particular version of the artifact
+    logger.info('Fetching raw dataset.')
+    local_path = wandb.use_artifact('sample.csv:latest').file()
+    df = pd.read_csv(local_path)
+    
+    # EDA with arguments passed into the step
+    logger.info('Cleaning data.')
+    idx = df['price'].between(float(args.min_price), float(args.max_price))
+    df = df[idx].copy()
+    df['last_review'] = pd.to_datetime(df['last_review'])
+    # TODO: add code to fix the issue happened when testing the model
+    idx = df['longitude'].between(-74.25, -73.50) & df['latitude'].between(40.5, 41.2)
+    df = df[idx].copy()
+
+    # Save the cleaned data
+    logger.info('Saving and exporting cleaned data.')
+    df.to_csv('clean_sample.csv', index=False)
+    artifact = wandb.Artifact(
+        args.output_artifact,
+        type = args.output_type,
+        description = args.output_description
+    )
+    artifact.add_file('clean_sample.csv')
+    run.log_artifact(artifact)
+    
+# TODO: In the code below, fill in the data type for each argumemt. The data type should be str, float or int. 
+# TODO: In the code below, fill in a description for each argument. The description should be a string.
+if __name__ == "__main__":
+
+    parser = argparse.ArgumentParser(description="A very basic data cleaning")
+  
+    parser.add_argument(
+        "--input_artifact", 
+        type = str,
+        help = "Name of the input artifact",
+        required = True
+    )
+
+    parser.add_argument(
+        "--output_artifact", 
+        type = str,
+        help = "Name of the output artifact",
+        required = True
+    )
+
+    parser.add_argument(
+        "--output_type", 
+        type = str,
+        help = "Type of the output artifact",
+        required = True
+    )
+
+    parser.add_argument(
+        "--output_description", 
+        type = str,
+        help = "Description of the output artifact",
+        required = True
+    )
+
+    parser.add_argument(
+        "--min_price", 
+        type = float,
+        help = "Minimum Price",
+        required = True
+    )
+
+    parser.add_argument(
+        "--max_price",
+        type = float,
+        help = "Maximum Price",
+        required = True
+    )
+
+
+    args = parser.parse_args()
+
+    go(args)
diff --git a/src/data_check/MLproject b/src/data_check/MLproject
index 8874479..7123c05 100644
--- a/src/data_check/MLproject
+++ b/src/data_check/MLproject
@@ -1,28 +1,28 @@
-name: data_check
-conda_env: conda.yml
-
-entry_points:
-  main:
-    parameters:
-
-      csv:
-        description: Input CSV file to be tested
-        type: string
-
-      ref:
-        description: Reference CSV file to compare the new csv to
-        type: string
-
-      kl_threshold:
-        description: Threshold for the KL divergence test on the neighborhood group column
-        type: float
-
-      min_price:
-        description: Minimum accepted price
-        type: float
-
-      max_price:
-        description: Maximum accepted price
-        type: float
-
-    command: "pytest . -vv --csv {csv} --ref {ref} --kl_threshold {kl_threshold} --min_price {min_price} --max_price {max_price}"
+name: data_check
+conda_env: conda.yml
+
+entry_points:
+  main:
+    parameters:
+
+      csv:
+        description: Input CSV file to be tested
+        type: string
+
+      ref:
+        description: Reference CSV file to compare the new csv to
+        type: string
+
+      kl_threshold:
+        description: Threshold for the KL divergence test on the neighborhood group column
+        type: float
+
+      min_price:
+        description: Minimum accepted price
+        type: float
+
+      max_price:
+        description: Maximum accepted price
+        type: float
+
+    command: "pytest . -vv --csv {csv} --ref {ref} --kl_threshold {kl_threshold} --min_price {min_price} --max_price {max_price}"
diff --git a/src/data_check/conda.yml b/src/data_check/conda.yml
index aa581fb..196b3a0 100644
--- a/src/data_check/conda.yml
+++ b/src/data_check/conda.yml
@@ -1,11 +1,11 @@
-name: data_check
-channels:
-  - conda-forge
-  - defaults
-dependencies:
-  - pandas=1.1.4
-  - pytest=6.2.2
-  - scipy=1.5.2
-  - pip=20.3.3
-  - pip:
-      - wandb==0.13.9
+name: data_check
+channels:
+  - conda-forge
+  - defaults
+dependencies:
+  - pandas=1.1.4
+  - pytest=6.2.2
+  - scipy=1.5.2
+  - pip=20.3.3
+  - pip:
+      - wandb==0.13.9
diff --git a/src/data_check/conftest.py b/src/data_check/conftest.py
index b000a45..a937a51 100644
--- a/src/data_check/conftest.py
+++ b/src/data_check/conftest.py
@@ -1,71 +1,71 @@
-import pytest
-import pandas as pd
-import wandb
-
-
-def pytest_addoption(parser):
-    parser.addoption("--csv", action="store")
-    parser.addoption("--ref", action="store")
-    parser.addoption("--kl_threshold", action="store")
-    parser.addoption("--min_price", action="store")
-    parser.addoption("--max_price", action="store")
-
-
-@pytest.fixture(scope='session')
-def data(request):
-    run = wandb.init(job_type="data_tests", resume=True)
-
-    # Download input artifact. This will also note that this script is using this
-    # particular version of the artifact
-    data_path = run.use_artifact(request.config.option.csv).file()
-
-    if data_path is None:
-        pytest.fail("You must provide the --csv option on the command line")
-
-    df = pd.read_csv(data_path)
-
-    return df
-
-
-@pytest.fixture(scope='session')
-def ref_data(request):
-    run = wandb.init(job_type="data_tests", resume=True)
-
-    # Download input artifact. This will also note that this script is using this
-    # particular version of the artifact
-    data_path = run.use_artifact(request.config.option.ref).file()
-
-    if data_path is None:
-        pytest.fail("You must provide the --ref option on the command line")
-
-    df = pd.read_csv(data_path)
-
-    return df
-
-
-@pytest.fixture(scope='session')
-def kl_threshold(request):
-    kl_threshold = request.config.option.kl_threshold
-
-    if kl_threshold is None:
-        pytest.fail("You must provide a threshold for the KL test")
-
-    return float(kl_threshold)
-
-@pytest.fixture(scope='session')
-def min_price(request):
-    min_price = request.config.option.min_price
-
-    if min_price is None:
-        pytest.fail("You must provide min_price")
-
-    return float(min_price)
-
-@pytest.fixture(scope='session')
-def max_price(request):
-    max_price = request.config.option.max_price
-
-    if max_price is None:
-        pytest.fail("You must provide max_price")
-
-    return float(max_price)
+import pytest
+import pandas as pd
+import wandb
+
+
+def pytest_addoption(parser):
+    parser.addoption("--csv", action="store")
+    parser.addoption("--ref", action="store")
+    parser.addoption("--kl_threshold", action="store")
+    parser.addoption("--min_price", action="store")
+    parser.addoption("--max_price", action="store")
+
+
+@pytest.fixture(scope='session')
+def data(request):
+    run = wandb.init(job_type="data_tests", resume=True)
+
+    # Download input artifact. This will also note that this script is using this
+    # particular version of the artifact
+    data_path = run.use_artifact(request.config.option.csv).file()
+
+    if data_path is None:
+        pytest.fail("You must provide the --csv option on the command line")
+
+    df = pd.read_csv(data_path)
+
+    return df
+
+
+@pytest.fixture(scope='session')
+def ref_data(request):
+    run = wandb.init(job_type="data_tests", resume=True)
+
+    # Download input artifact. This will also note that this script is using this
+    # particular version of the artifact
+    data_path = run.use_artifact(request.config.option.ref).file()
+
+    if data_path is None:
+        pytest.fail("You must provide the --ref option on the command line")
+
+    df = pd.read_csv(data_path)
+
+    return df
+
+
+@pytest.fixture(scope='session')
+def kl_threshold(request):
+    kl_threshold = request.config.option.kl_threshold
+
+    if kl_threshold is None:
+        pytest.fail("You must provide a threshold for the KL test")
+
+    return float(kl_threshold)
+
+@pytest.fixture(scope='session')
+def min_price(request):
+    min_price = request.config.option.min_price
+
+    if min_price is None:
+        pytest.fail("You must provide min_price")
+
+    return float(min_price)
+
+@pytest.fixture(scope='session')
+def max_price(request):
+    max_price = request.config.option.max_price
+
+    if max_price is None:
+        pytest.fail("You must provide max_price")
+
+    return float(max_price)
diff --git a/src/data_check/test_data.py b/src/data_check/test_data.py
index 6ed3ec6..b52677f 100644
--- a/src/data_check/test_data.py
+++ b/src/data_check/test_data.py
@@ -1,65 +1,71 @@
-import pandas as pd
-import numpy as np
-import scipy.stats
-
-
-def test_column_names(data):
-
-    expected_colums = [
-        "id",
-        "name",
-        "host_id",
-        "host_name",
-        "neighbourhood_group",
-        "neighbourhood",
-        "latitude",
-        "longitude",
-        "room_type",
-        "price",
-        "minimum_nights",
-        "number_of_reviews",
-        "last_review",
-        "reviews_per_month",
-        "calculated_host_listings_count",
-        "availability_365",
-    ]
-
-    these_columns = data.columns.values
-
-    # This also enforces the same order
-    assert list(expected_colums) == list(these_columns)
-
-
-def test_neighborhood_names(data):
-
-    known_names = ["Bronx", "Brooklyn", "Manhattan", "Queens", "Staten Island"]
-
-    neigh = set(data['neighbourhood_group'].unique())
-
-    # Unordered check
-    assert set(known_names) == set(neigh)
-
-
-def test_proper_boundaries(data: pd.DataFrame):
-    """
-    Test proper longitude and latitude boundaries for properties in and around NYC
-    """
-    idx = data['longitude'].between(-74.25, -73.50) & data['latitude'].between(40.5, 41.2)
-
-    assert np.sum(~idx) == 0
-
-
-def test_similar_neigh_distrib(data: pd.DataFrame, ref_data: pd.DataFrame, kl_threshold: float):
-    """
-    Apply a threshold on the KL divergence to detect if the distribution of the new data is
-    significantly different than that of the reference dataset
-    """
-    dist1 = data['neighbourhood_group'].value_counts().sort_index()
-    dist2 = ref_data['neighbourhood_group'].value_counts().sort_index()
-
-    assert scipy.stats.entropy(dist1, dist2, base=2) < kl_threshold
-
-
-########################################################
-# Implement here test_row_count and test_price_range   #
-########################################################
+import pandas as pd
+import numpy as np
+import scipy.stats
+
+
+def test_column_names(data):
+
+    expected_colums = [
+        "id",
+        "name",
+        "host_id",
+        "host_name",
+        "neighbourhood_group",
+        "neighbourhood",
+        "latitude",
+        "longitude",
+        "room_type",
+        "price",
+        "minimum_nights",
+        "number_of_reviews",
+        "last_review",
+        "reviews_per_month",
+        "calculated_host_listings_count",
+        "availability_365",
+    ]
+
+    these_columns = data.columns.values
+
+    # This also enforces the same order
+    assert list(expected_colums) == list(these_columns)
+
+
+def test_neighborhood_names(data):
+
+    known_names = ["Bronx", "Brooklyn", "Manhattan", "Queens", "Staten Island"]
+
+    neigh = set(data['neighbourhood_group'].unique())
+
+    # Unordered check
+    assert set(known_names) == set(neigh)
+
+
+def test_proper_boundaries(data: pd.DataFrame):
+    """
+    Test proper longitude and latitude boundaries for properties in and around NYC
+    """
+    idx = data['longitude'].between(-74.25, -73.50) & data['latitude'].between(40.5, 41.2)
+
+    assert np.sum(~idx) == 0
+
+
+def test_similar_neigh_distrib(data: pd.DataFrame, ref_data: pd.DataFrame, kl_threshold: float):
+    """
+    Apply a threshold on the KL divergence to detect if the distribution of the new data is
+    significantly different than that of the reference dataset
+    """
+    dist1 = data['neighbourhood_group'].value_counts().sort_index()
+    dist2 = ref_data['neighbourhood_group'].value_counts().sort_index()
+
+    assert scipy.stats.entropy(dist1, dist2, base=2) < kl_threshold
+
+
+########################################################
+# Implement here test_row_count and test_price_range   #
+########################################################
+def test_row_count(data: pd.DataFrame):
+    assert 15000 < data.shape[0] < 1000000
+
+
+def test_price_range(data, min_price, max_price):
+    assert data['price'].between(min_price, max_price).all()
\ No newline at end of file
diff --git a/src/eda/eda.ipynb b/src/eda/eda.ipynb
index 06abc38..6e28e88 100644
--- a/src/eda/eda.ipynb
+++ b/src/eda/eda.ipynb
@@ -2,8 +2,8 @@
  "cells": [
   {
    "cell_type": "code",
-   "execution_count": 6,
-   "id": "1f72a955",
+   "execution_count": 1,
+   "id": "a9fe5ede",
    "metadata": {
     "scrolled": true,
     "tags": []
@@ -13,72 +13,134 @@
      "name": "stdout",
      "output_type": "stream",
      "text": [
-      "Requirement already satisfied: wandb==0.13.9 in /opt/anaconda3/envs/nyc_airbnb_dev/lib/python3.9/site-packages (0.13.9)\n",
-      "Requirement already satisfied: setproctitle in /opt/anaconda3/envs/nyc_airbnb_dev/lib/python3.9/site-packages (from wandb==0.13.9) (1.3.2)\n",
-      "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /opt/anaconda3/envs/nyc_airbnb_dev/lib/python3.9/site-packages (from wandb==0.13.9) (4.21.12)\n",
-      "Requirement already satisfied: docker-pycreds>=0.4.0 in /opt/anaconda3/envs/nyc_airbnb_dev/lib/python3.9/site-packages (from wandb==0.13.9) (0.4.0)\n",
-      "Requirement already satisfied: setuptools in /opt/anaconda3/envs/nyc_airbnb_dev/lib/python3.9/site-packages (from wandb==0.13.9) (59.8.0)\n",
-      "Requirement already satisfied: pathtools in /opt/anaconda3/envs/nyc_airbnb_dev/lib/python3.9/site-packages (from wandb==0.13.9) (0.1.2)\n",
-      "Requirement already satisfied: Click!=8.0.0,>=7.0 in /opt/anaconda3/envs/nyc_airbnb_dev/lib/python3.9/site-packages (from wandb==0.13.9) (8.1.3)\n",
-      "Requirement already satisfied: GitPython>=1.0.0 in /opt/anaconda3/envs/nyc_airbnb_dev/lib/python3.9/site-packages (from wandb==0.13.9) (3.1.31)\n",
-      "Requirement already satisfied: appdirs>=1.4.3 in /opt/anaconda3/envs/nyc_airbnb_dev/lib/python3.9/site-packages (from wandb==0.13.9) (1.4.4)\n",
-      "Requirement already satisfied: requests<3,>=2.0.0 in /opt/anaconda3/envs/nyc_airbnb_dev/lib/python3.9/site-packages (from wandb==0.13.9) (2.28.2)\n",
-      "Requirement already satisfied: typing-extensions in /opt/anaconda3/envs/nyc_airbnb_dev/lib/python3.9/site-packages (from wandb==0.13.9) (4.5.0)\n",
-      "Requirement already satisfied: PyYAML in /opt/anaconda3/envs/nyc_airbnb_dev/lib/python3.9/site-packages (from wandb==0.13.9) (6.0)\n",
-      "Requirement already satisfied: sentry-sdk>=1.0.0 in /opt/anaconda3/envs/nyc_airbnb_dev/lib/python3.9/site-packages (from wandb==0.13.9) (1.19.1)\n",
-      "Requirement already satisfied: psutil>=5.0.0 in /opt/anaconda3/envs/nyc_airbnb_dev/lib/python3.9/site-packages (from wandb==0.13.9) (5.9.4)\n",
-      "Requirement already satisfied: six>=1.4.0 in /opt/anaconda3/envs/nyc_airbnb_dev/lib/python3.9/site-packages (from docker-pycreds>=0.4.0->wandb==0.13.9) (1.16.0)\n",
-      "Requirement already satisfied: gitdb<5,>=4.0.1 in /opt/anaconda3/envs/nyc_airbnb_dev/lib/python3.9/site-packages (from GitPython>=1.0.0->wandb==0.13.9) (4.0.10)\n",
-      "Requirement already satisfied: smmap<6,>=3.0.1 in /opt/anaconda3/envs/nyc_airbnb_dev/lib/python3.9/site-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb==0.13.9) (3.0.5)\n",
-      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/envs/nyc_airbnb_dev/lib/python3.9/site-packages (from requests<3,>=2.0.0->wandb==0.13.9) (3.4)\n",
-      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/anaconda3/envs/nyc_airbnb_dev/lib/python3.9/site-packages (from requests<3,>=2.0.0->wandb==0.13.9) (1.26.15)\n",
-      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/envs/nyc_airbnb_dev/lib/python3.9/site-packages (from requests<3,>=2.0.0->wandb==0.13.9) (3.1.0)\n",
-      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/envs/nyc_airbnb_dev/lib/python3.9/site-packages (from requests<3,>=2.0.0->wandb==0.13.9) (2022.12.7)\n",
-      "Requirement already satisfied: pandas-profiling==3.6.2 in /opt/anaconda3/envs/nyc_airbnb_dev/lib/python3.9/site-packages (3.6.2)\n",
-      "Requirement already satisfied: multimethod<1.10,>=1.4 in /opt/anaconda3/envs/nyc_airbnb_dev/lib/python3.9/site-packages (from pandas-profiling==3.6.2) (1.9.1)\n",
-      "Requirement already satisfied: jinja2<3.2,>=2.11.1 in /opt/anaconda3/envs/nyc_airbnb_dev/lib/python3.9/site-packages (from pandas-profiling==3.6.2) (2.11.3)\n",
-      "Requirement already satisfied: matplotlib<3.7,>=3.2 in /opt/anaconda3/envs/nyc_airbnb_dev/lib/python3.9/site-packages (from pandas-profiling==3.6.2) (3.6.2)\n",
-      "Requirement already satisfied: scipy<1.10,>=1.4.1 in /opt/anaconda3/envs/nyc_airbnb_dev/lib/python3.9/site-packages (from pandas-profiling==3.6.2) (1.9.3)\n",
-      "Requirement already satisfied: htmlmin==0.1.12 in /opt/anaconda3/envs/nyc_airbnb_dev/lib/python3.9/site-packages (from pandas-profiling==3.6.2) (0.1.12)\n",
-      "Requirement already satisfied: requests<2.29,>=2.24.0 in /opt/anaconda3/envs/nyc_airbnb_dev/lib/python3.9/site-packages (from pandas-profiling==3.6.2) (2.28.2)\n",
-      "Requirement already satisfied: numpy<1.24,>=1.16.0 in /opt/anaconda3/envs/nyc_airbnb_dev/lib/python3.9/site-packages (from pandas-profiling==3.6.2) (1.23.5)\n",
-      "Requirement already satisfied: seaborn<0.13,>=0.10.1 in /opt/anaconda3/envs/nyc_airbnb_dev/lib/python3.9/site-packages (from pandas-profiling==3.6.2) (0.11.1)\n",
-      "Requirement already satisfied: statsmodels<0.14,>=0.13.2 in /opt/anaconda3/envs/nyc_airbnb_dev/lib/python3.9/site-packages (from pandas-profiling==3.6.2) (0.13.5)\n",
-      "Requirement already satisfied: pydantic<1.11,>=1.8.1 in /opt/anaconda3/envs/nyc_airbnb_dev/lib/python3.9/site-packages (from pandas-profiling==3.6.2) (1.10.7)\n",
-      "Requirement already satisfied: pandas!=1.4.0,<1.6,>1.1 in /opt/anaconda3/envs/nyc_airbnb_dev/lib/python3.9/site-packages (from pandas-profiling==3.6.2) (1.2.3)\n",
-      "Requirement already satisfied: phik<0.13,>=0.11.1 in /opt/anaconda3/envs/nyc_airbnb_dev/lib/python3.9/site-packages (from pandas-profiling==3.6.2) (0.12.3)\n",
-      "Requirement already satisfied: typeguard<2.14,>=2.13.2 in /opt/anaconda3/envs/nyc_airbnb_dev/lib/python3.9/site-packages (from pandas-profiling==3.6.2) (2.13.3)\n",
-      "Requirement already satisfied: PyYAML<6.1,>=5.0.0 in /opt/anaconda3/envs/nyc_airbnb_dev/lib/python3.9/site-packages (from pandas-profiling==3.6.2) (6.0)\n",
-      "Requirement already satisfied: visions[type_image_path]==0.7.5 in /opt/anaconda3/envs/nyc_airbnb_dev/lib/python3.9/site-packages (from pandas-profiling==3.6.2) (0.7.5)\n",
-      "Requirement already satisfied: tqdm<4.65,>=4.48.2 in /opt/anaconda3/envs/nyc_airbnb_dev/lib/python3.9/site-packages (from pandas-profiling==3.6.2) (4.64.1)\n",
-      "Requirement already satisfied: attrs>=19.3.0 in /opt/anaconda3/envs/nyc_airbnb_dev/lib/python3.9/site-packages (from visions[type_image_path]==0.7.5->pandas-profiling==3.6.2) (22.2.0)\n",
-      "Requirement already satisfied: networkx>=2.4 in /opt/anaconda3/envs/nyc_airbnb_dev/lib/python3.9/site-packages (from visions[type_image_path]==0.7.5->pandas-profiling==3.6.2) (3.1)\n",
-      "Requirement already satisfied: tangled-up-in-unicode>=0.0.4 in /opt/anaconda3/envs/nyc_airbnb_dev/lib/python3.9/site-packages (from visions[type_image_path]==0.7.5->pandas-profiling==3.6.2) (0.2.0)\n",
-      "Requirement already satisfied: Pillow in /opt/anaconda3/envs/nyc_airbnb_dev/lib/python3.9/site-packages (from visions[type_image_path]==0.7.5->pandas-profiling==3.6.2) (9.5.0)\n",
-      "Requirement already satisfied: imagehash in /opt/anaconda3/envs/nyc_airbnb_dev/lib/python3.9/site-packages (from visions[type_image_path]==0.7.5->pandas-profiling==3.6.2) (4.3.1)\n",
-      "Requirement already satisfied: MarkupSafe>=0.23 in /opt/anaconda3/envs/nyc_airbnb_dev/lib/python3.9/site-packages (from jinja2<3.2,>=2.11.1->pandas-profiling==3.6.2) (1.1.1)\n",
-      "Requirement already satisfied: pyparsing>=2.2.1 in /opt/anaconda3/envs/nyc_airbnb_dev/lib/python3.9/site-packages (from matplotlib<3.7,>=3.2->pandas-profiling==3.6.2) (3.0.9)\n",
-      "Requirement already satisfied: python-dateutil>=2.7 in /opt/anaconda3/envs/nyc_airbnb_dev/lib/python3.9/site-packages (from matplotlib<3.7,>=3.2->pandas-profiling==3.6.2) (2.8.2)\n",
-      "Requirement already satisfied: contourpy>=1.0.1 in /opt/anaconda3/envs/nyc_airbnb_dev/lib/python3.9/site-packages (from matplotlib<3.7,>=3.2->pandas-profiling==3.6.2) (1.0.7)\n",
-      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/envs/nyc_airbnb_dev/lib/python3.9/site-packages (from matplotlib<3.7,>=3.2->pandas-profiling==3.6.2) (22.0)\n",
-      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/anaconda3/envs/nyc_airbnb_dev/lib/python3.9/site-packages (from matplotlib<3.7,>=3.2->pandas-profiling==3.6.2) (1.4.4)\n",
-      "Requirement already satisfied: fonttools>=4.22.0 in /opt/anaconda3/envs/nyc_airbnb_dev/lib/python3.9/site-packages (from matplotlib<3.7,>=3.2->pandas-profiling==3.6.2) (4.39.3)\n",
-      "Requirement already satisfied: cycler>=0.10 in /opt/anaconda3/envs/nyc_airbnb_dev/lib/python3.9/site-packages (from matplotlib<3.7,>=3.2->pandas-profiling==3.6.2) (0.11.0)\n",
-      "Requirement already satisfied: pytz>=2017.3 in /opt/anaconda3/envs/nyc_airbnb_dev/lib/python3.9/site-packages (from pandas!=1.4.0,<1.6,>1.1->pandas-profiling==3.6.2) (2022.7.1)\n",
-      "Requirement already satisfied: joblib>=0.14.1 in /opt/anaconda3/envs/nyc_airbnb_dev/lib/python3.9/site-packages (from phik<0.13,>=0.11.1->pandas-profiling==3.6.2) (1.2.0)\n",
-      "Requirement already satisfied: typing-extensions>=4.2.0 in /opt/anaconda3/envs/nyc_airbnb_dev/lib/python3.9/site-packages (from pydantic<1.11,>=1.8.1->pandas-profiling==3.6.2) (4.5.0)\n",
-      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/envs/nyc_airbnb_dev/lib/python3.9/site-packages (from python-dateutil>=2.7->matplotlib<3.7,>=3.2->pandas-profiling==3.6.2) (1.16.0)\n",
-      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/envs/nyc_airbnb_dev/lib/python3.9/site-packages (from requests<2.29,>=2.24.0->pandas-profiling==3.6.2) (2022.12.7)\n",
-      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/anaconda3/envs/nyc_airbnb_dev/lib/python3.9/site-packages (from requests<2.29,>=2.24.0->pandas-profiling==3.6.2) (1.26.15)\n",
-      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/envs/nyc_airbnb_dev/lib/python3.9/site-packages (from requests<2.29,>=2.24.0->pandas-profiling==3.6.2) (3.1.0)\n",
-      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/envs/nyc_airbnb_dev/lib/python3.9/site-packages (from requests<2.29,>=2.24.0->pandas-profiling==3.6.2) (3.4)\n",
-      "Requirement already satisfied: patsy>=0.5.2 in /opt/anaconda3/envs/nyc_airbnb_dev/lib/python3.9/site-packages (from statsmodels<0.14,>=0.13.2->pandas-profiling==3.6.2) (0.5.3)\n",
-      "Requirement already satisfied: PyWavelets in /opt/anaconda3/envs/nyc_airbnb_dev/lib/python3.9/site-packages (from imagehash->visions[type_image_path]==0.7.5->pandas-profiling==3.6.2) (1.4.1)\n",
-      "Requirement already satisfied: pandas==1.2.3 in /opt/anaconda3/envs/nyc_airbnb_dev/lib/python3.9/site-packages (1.2.3)\n",
-      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/anaconda3/envs/nyc_airbnb_dev/lib/python3.9/site-packages (from pandas==1.2.3) (2.8.2)\n",
-      "Requirement already satisfied: pytz>=2017.3 in /opt/anaconda3/envs/nyc_airbnb_dev/lib/python3.9/site-packages (from pandas==1.2.3) (2022.7.1)\n",
-      "Requirement already satisfied: numpy>=1.16.5 in /opt/anaconda3/envs/nyc_airbnb_dev/lib/python3.9/site-packages (from pandas==1.2.3) (1.23.5)\n",
-      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/envs/nyc_airbnb_dev/lib/python3.9/site-packages (from python-dateutil>=2.7.3->pandas==1.2.3) (1.16.0)\n"
+      "Requirement already satisfied: wandb==0.13.9 in /home/kyle/miniconda3/envs/nyc_airbnb_dev/lib/python3.9/site-packages (0.13.9)\n",
+      "Requirement already satisfied: sentry-sdk>=1.0.0 in /home/kyle/miniconda3/envs/nyc_airbnb_dev/lib/python3.9/site-packages (from wandb==0.13.9) (1.40.0)\n",
+      "Requirement already satisfied: psutil>=5.0.0 in /home/kyle/miniconda3/envs/nyc_airbnb_dev/lib/python3.9/site-packages (from wandb==0.13.9) (5.9.8)\n",
+      "Requirement already satisfied: pathtools in /home/kyle/miniconda3/envs/nyc_airbnb_dev/lib/python3.9/site-packages (from wandb==0.13.9) (0.1.2)\n",
+      "Requirement already satisfied: appdirs>=1.4.3 in /home/kyle/miniconda3/envs/nyc_airbnb_dev/lib/python3.9/site-packages (from wandb==0.13.9) (1.4.4)\n",
+      "Requirement already satisfied: setproctitle in /home/kyle/miniconda3/envs/nyc_airbnb_dev/lib/python3.9/site-packages (from wandb==0.13.9) (1.3.3)\n",
+      "Requirement already satisfied: setuptools in /home/kyle/miniconda3/envs/nyc_airbnb_dev/lib/python3.9/site-packages (from wandb==0.13.9) (59.8.0)\n",
+      "Requirement already satisfied: requests<3,>=2.0.0 in /home/kyle/miniconda3/envs/nyc_airbnb_dev/lib/python3.9/site-packages (from wandb==0.13.9) (2.31.0)\n",
+      "Requirement already satisfied: GitPython>=1.0.0 in /home/kyle/miniconda3/envs/nyc_airbnb_dev/lib/python3.9/site-packages (from wandb==0.13.9) (3.1.41)\n",
+      "Requirement already satisfied: typing-extensions in /home/kyle/miniconda3/envs/nyc_airbnb_dev/lib/python3.9/site-packages (from wandb==0.13.9) (4.9.0)\n",
+      "Requirement already satisfied: Click!=8.0.0,>=7.0 in /home/kyle/miniconda3/envs/nyc_airbnb_dev/lib/python3.9/site-packages (from wandb==0.13.9) (8.1.7)\n",
+      "Requirement already satisfied: PyYAML in /home/kyle/miniconda3/envs/nyc_airbnb_dev/lib/python3.9/site-packages (from wandb==0.13.9) (6.0.1)\n",
+      "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.15.0 in /home/kyle/miniconda3/envs/nyc_airbnb_dev/lib/python3.9/site-packages (from wandb==0.13.9) (4.21.12)\n",
+      "Requirement already satisfied: docker-pycreds>=0.4.0 in /home/kyle/miniconda3/envs/nyc_airbnb_dev/lib/python3.9/site-packages (from wandb==0.13.9) (0.4.0)\n",
+      "Requirement already satisfied: six>=1.4.0 in /home/kyle/miniconda3/envs/nyc_airbnb_dev/lib/python3.9/site-packages (from docker-pycreds>=0.4.0->wandb==0.13.9) (1.16.0)\n",
+      "Requirement already satisfied: gitdb<5,>=4.0.1 in /home/kyle/miniconda3/envs/nyc_airbnb_dev/lib/python3.9/site-packages (from GitPython>=1.0.0->wandb==0.13.9) (4.0.11)\n",
+      "Requirement already satisfied: smmap<6,>=3.0.1 in /home/kyle/miniconda3/envs/nyc_airbnb_dev/lib/python3.9/site-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb==0.13.9) (5.0.0)\n",
+      "Requirement already satisfied: certifi>=2017.4.17 in /home/kyle/miniconda3/envs/nyc_airbnb_dev/lib/python3.9/site-packages (from requests<3,>=2.0.0->wandb==0.13.9) (2023.11.17)\n",
+      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/kyle/miniconda3/envs/nyc_airbnb_dev/lib/python3.9/site-packages (from requests<3,>=2.0.0->wandb==0.13.9) (3.3.2)\n",
+      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/kyle/miniconda3/envs/nyc_airbnb_dev/lib/python3.9/site-packages (from requests<3,>=2.0.0->wandb==0.13.9) (2.2.0)\n",
+      "Requirement already satisfied: idna<4,>=2.5 in /home/kyle/miniconda3/envs/nyc_airbnb_dev/lib/python3.9/site-packages (from requests<3,>=2.0.0->wandb==0.13.9) (3.6)\n",
+      "Collecting pandas-profiling==3.6.2\n",
+      "  Downloading pandas_profiling-3.6.2-py2.py3-none-any.whl (328 kB)\n",
+      "\u001b[K     |████████████████████████████████| 328 kB 657 kB/s eta 0:00:01\n",
+      "\u001b[?25hRequirement already satisfied: PyYAML<6.1,>=5.0.0 in /home/kyle/miniconda3/envs/nyc_airbnb_dev/lib/python3.9/site-packages (from pandas-profiling==3.6.2) (6.0.1)\n",
+      "Requirement already satisfied: jinja2<3.2,>=2.11.1 in /home/kyle/miniconda3/envs/nyc_airbnb_dev/lib/python3.9/site-packages (from pandas-profiling==3.6.2) (2.11.3)\n",
+      "Requirement already satisfied: pandas!=1.4.0,<1.6,>1.1 in /home/kyle/miniconda3/envs/nyc_airbnb_dev/lib/python3.9/site-packages (from pandas-profiling==3.6.2) (1.2.3)\n",
+      "Requirement already satisfied: matplotlib<3.7,>=3.2 in /home/kyle/miniconda3/envs/nyc_airbnb_dev/lib/python3.9/site-packages (from pandas-profiling==3.6.2) (3.6.2)\n",
+      "Collecting htmlmin==0.1.12\n",
+      "  Downloading htmlmin-0.1.12.tar.gz (19 kB)\n",
+      "Collecting visions[type_image_path]==0.7.5\n",
+      "  Downloading visions-0.7.5-py3-none-any.whl (102 kB)\n",
+      "\u001b[K     |████████████████████████████████| 102 kB 1.1 MB/s eta 0:00:01\n",
+      "\u001b[?25hRequirement already satisfied: attrs>=19.3.0 in /home/kyle/miniconda3/envs/nyc_airbnb_dev/lib/python3.9/site-packages (from visions[type_image_path]==0.7.5->pandas-profiling==3.6.2) (23.2.0)\n",
+      "Requirement already satisfied: Pillow in /home/kyle/miniconda3/envs/nyc_airbnb_dev/lib/python3.9/site-packages (from visions[type_image_path]==0.7.5->pandas-profiling==3.6.2) (9.4.0)\n",
+      "Requirement already satisfied: MarkupSafe>=0.23 in /home/kyle/miniconda3/envs/nyc_airbnb_dev/lib/python3.9/site-packages (from jinja2<3.2,>=2.11.1->pandas-profiling==3.6.2) (1.1.1)\n",
+      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/kyle/miniconda3/envs/nyc_airbnb_dev/lib/python3.9/site-packages (from matplotlib<3.7,>=3.2->pandas-profiling==3.6.2) (1.4.5)\n",
+      "Requirement already satisfied: fonttools>=4.22.0 in /home/kyle/miniconda3/envs/nyc_airbnb_dev/lib/python3.9/site-packages (from matplotlib<3.7,>=3.2->pandas-profiling==3.6.2) (4.47.2)\n",
+      "Requirement already satisfied: packaging>=20.0 in /home/kyle/miniconda3/envs/nyc_airbnb_dev/lib/python3.9/site-packages (from matplotlib<3.7,>=3.2->pandas-profiling==3.6.2) (22.0)\n",
+      "Requirement already satisfied: python-dateutil>=2.7 in /home/kyle/miniconda3/envs/nyc_airbnb_dev/lib/python3.9/site-packages (from matplotlib<3.7,>=3.2->pandas-profiling==3.6.2) (2.8.2)\n",
+      "Requirement already satisfied: contourpy>=1.0.1 in /home/kyle/miniconda3/envs/nyc_airbnb_dev/lib/python3.9/site-packages (from matplotlib<3.7,>=3.2->pandas-profiling==3.6.2) (1.2.0)\n",
+      "Requirement already satisfied: cycler>=0.10 in /home/kyle/miniconda3/envs/nyc_airbnb_dev/lib/python3.9/site-packages (from matplotlib<3.7,>=3.2->pandas-profiling==3.6.2) (0.12.1)\n",
+      "Requirement already satisfied: pyparsing>=2.2.1 in /home/kyle/miniconda3/envs/nyc_airbnb_dev/lib/python3.9/site-packages (from matplotlib<3.7,>=3.2->pandas-profiling==3.6.2) (3.1.1)\n",
+      "Collecting multimethod<1.10,>=1.4\n",
+      "  Downloading multimethod-1.9.1-py3-none-any.whl (10 kB)\n",
+      "Collecting networkx>=2.4\n",
+      "  Downloading networkx-3.2.1-py3-none-any.whl (1.6 MB)\n",
+      "\u001b[K     |████████████████████████████████| 1.6 MB 1.1 MB/s eta 0:00:01\n",
+      "\u001b[?25hCollecting numpy<1.24,>=1.16.0\n",
+      "  Downloading numpy-1.23.5-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.1 MB)\n",
+      "\u001b[K     |████████████████████████████████| 17.1 MB 1.0 MB/s eta 0:00:01    |████████▋                       | 4.6 MB 893 kB/s eta 0:00:14\n",
+      "\u001b[?25hRequirement already satisfied: pytz>=2017.3 in /home/kyle/miniconda3/envs/nyc_airbnb_dev/lib/python3.9/site-packages (from pandas!=1.4.0,<1.6,>1.1->pandas-profiling==3.6.2) (2022.7.1)\n",
+      "Collecting phik<0.13,>=0.11.1\n",
+      "  Downloading phik-0.12.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (686 kB)\n",
+      "\u001b[K     |████████████████████████████████| 686 kB 1.1 MB/s eta 0:00:01\n",
+      "\u001b[?25hRequirement already satisfied: joblib>=0.14.1 in /home/kyle/miniconda3/envs/nyc_airbnb_dev/lib/python3.9/site-packages (from phik<0.13,>=0.11.1->pandas-profiling==3.6.2) (1.3.2)\n",
+      "Collecting pydantic<1.11,>=1.8.1\n",
+      "  Downloading pydantic-1.10.14-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.2 MB)\n",
+      "\u001b[K     |████████████████████████████████| 3.2 MB 219 kB/s eta 0:00:01\n",
+      "\u001b[?25hRequirement already satisfied: typing-extensions>=4.2.0 in /home/kyle/miniconda3/envs/nyc_airbnb_dev/lib/python3.9/site-packages (from pydantic<1.11,>=1.8.1->pandas-profiling==3.6.2) (4.9.0)\n",
+      "Requirement already satisfied: six>=1.5 in /home/kyle/miniconda3/envs/nyc_airbnb_dev/lib/python3.9/site-packages (from python-dateutil>=2.7->matplotlib<3.7,>=3.2->pandas-profiling==3.6.2) (1.16.0)\n",
+      "Collecting requests<2.29,>=2.24.0\n",
+      "  Downloading requests-2.28.2-py3-none-any.whl (62 kB)\n",
+      "\u001b[K     |████████████████████████████████| 62 kB 371 kB/s eta 0:00:01\n",
+      "\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /home/kyle/miniconda3/envs/nyc_airbnb_dev/lib/python3.9/site-packages (from requests<2.29,>=2.24.0->pandas-profiling==3.6.2) (2023.11.17)\n",
+      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/kyle/miniconda3/envs/nyc_airbnb_dev/lib/python3.9/site-packages (from requests<2.29,>=2.24.0->pandas-profiling==3.6.2) (3.3.2)\n",
+      "Requirement already satisfied: idna<4,>=2.5 in /home/kyle/miniconda3/envs/nyc_airbnb_dev/lib/python3.9/site-packages (from requests<2.29,>=2.24.0->pandas-profiling==3.6.2) (3.6)\n",
+      "Collecting scipy<1.10,>=1.4.1\n",
+      "  Downloading scipy-1.9.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (33.8 MB)\n",
+      "\u001b[K     |████████████████████████████████| 33.8 MB 1.3 MB/s eta 0:00:01    |▏                               | 122 kB 1.1 MB/s eta 0:00:30\n",
+      "\u001b[?25hCollecting seaborn<0.13,>=0.10.1\n",
+      "  Downloading seaborn-0.12.2-py3-none-any.whl (293 kB)\n",
+      "\u001b[K     |████████████████████████████████| 293 kB 1.0 MB/s eta 0:00:01\n",
+      "\u001b[?25hCollecting statsmodels<0.14,>=0.13.2\n",
+      "  Downloading statsmodels-0.13.5-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.9 MB)\n",
+      "\u001b[K     |████████████████████████████████| 9.9 MB 1.3 MB/s eta 0:00:01     |█████████████████████████▏      | 7.8 MB 1.4 MB/s eta 0:00:02\n",
+      "\u001b[?25hCollecting patsy>=0.5.2\n",
+      "  Downloading patsy-0.5.6-py2.py3-none-any.whl (233 kB)\n",
+      "\u001b[K     |████████████████████████████████| 233 kB 1.3 MB/s eta 0:00:01\n",
+      "\u001b[?25hCollecting tangled-up-in-unicode>=0.0.4\n",
+      "  Downloading tangled_up_in_unicode-0.2.0-py3-none-any.whl (4.7 MB)\n",
+      "\u001b[K     |████████████████████████████████| 4.7 MB 1.4 MB/s eta 0:00:01\n",
+      "\u001b[?25hCollecting tqdm<4.65,>=4.48.2\n",
+      "  Downloading tqdm-4.64.1-py2.py3-none-any.whl (78 kB)\n",
+      "\u001b[K     |████████████████████████████████| 78 kB 895 kB/s eta 0:00:01\n",
+      "\u001b[?25hCollecting typeguard<2.14,>=2.13.2\n",
+      "  Downloading typeguard-2.13.3-py3-none-any.whl (17 kB)\n",
+      "Collecting urllib3<1.27,>=1.21.1\n",
+      "  Downloading urllib3-1.26.18-py2.py3-none-any.whl (143 kB)\n",
+      "\u001b[K     |████████████████████████████████| 143 kB 1.1 MB/s eta 0:00:01\n",
+      "\u001b[?25hCollecting imagehash\n",
+      "  Downloading ImageHash-4.3.1-py2.py3-none-any.whl (296 kB)\n",
+      "\u001b[K     |████████████████████████████████| 296 kB 1.0 MB/s eta 0:00:01\n",
+      "\u001b[?25hCollecting PyWavelets\n",
+      "  Downloading pywavelets-1.5.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.5 MB)\n",
+      "\u001b[K     |████████████████████████████████| 4.5 MB 1.2 MB/s eta 0:00:01\n",
+      "\u001b[?25hBuilding wheels for collected packages: htmlmin\n",
+      "  Building wheel for htmlmin (setup.py) ... \u001b[?25ldone\n",
+      "\u001b[?25h  Created wheel for htmlmin: filename=htmlmin-0.1.12-py3-none-any.whl size=27099 sha256=a2ec48faca3184d55b50b8e71ec6fc96eedfa6d954b914cc547bf3292fce2fa2\n",
+      "  Stored in directory: /home/kyle/.cache/pip/wheels/1d/05/04/c6d7d3b66539d9e659ac6dfe81e2d0fd4c1a8316cc5a403300\n",
+      "Successfully built htmlmin\n",
+      "Installing collected packages: numpy, tangled-up-in-unicode, scipy, PyWavelets, networkx, multimethod, visions, urllib3, patsy, imagehash, typeguard, tqdm, statsmodels, seaborn, requests, pydantic, phik, htmlmin, pandas-profiling\n",
+      "  Attempting uninstall: numpy\n",
+      "    Found existing installation: numpy 1.24.1\n",
+      "    Uninstalling numpy-1.24.1:\n",
+      "      Successfully uninstalled numpy-1.24.1\n",
+      "  Attempting uninstall: scipy\n",
+      "    Found existing installation: scipy 1.12.0\n",
+      "    Uninstalling scipy-1.12.0:\n",
+      "      Successfully uninstalled scipy-1.12.0\n",
+      "  Attempting uninstall: urllib3\n",
+      "    Found existing installation: urllib3 2.2.0\n",
+      "    Uninstalling urllib3-2.2.0:\n",
+      "      Successfully uninstalled urllib3-2.2.0\n",
+      "  Attempting uninstall: tqdm\n",
+      "    Found existing installation: tqdm 4.66.1\n",
+      "    Uninstalling tqdm-4.66.1:\n",
+      "      Successfully uninstalled tqdm-4.66.1\n",
+      "  Attempting uninstall: requests\n",
+      "    Found existing installation: requests 2.31.0\n",
+      "    Uninstalling requests-2.31.0:\n",
+      "      Successfully uninstalled requests-2.31.0\n",
+      "Successfully installed PyWavelets-1.5.0 htmlmin-0.1.12 imagehash-4.3.1 multimethod-1.9.1 networkx-3.2.1 numpy-1.23.5 pandas-profiling-3.6.2 patsy-0.5.6 phik-0.12.4 pydantic-1.10.14 requests-2.28.2 scipy-1.9.3 seaborn-0.12.2 statsmodels-0.13.5 tangled-up-in-unicode-0.2.0 tqdm-4.64.1 typeguard-2.13.3 urllib3-1.26.18 visions-0.7.5\n",
+      "Requirement already satisfied: pandas==1.2.3 in /home/kyle/miniconda3/envs/nyc_airbnb_dev/lib/python3.9/site-packages (1.2.3)\n",
+      "Requirement already satisfied: python-dateutil>=2.7.3 in /home/kyle/miniconda3/envs/nyc_airbnb_dev/lib/python3.9/site-packages (from pandas==1.2.3) (2.8.2)\n",
+      "Requirement already satisfied: pytz>=2017.3 in /home/kyle/miniconda3/envs/nyc_airbnb_dev/lib/python3.9/site-packages (from pandas==1.2.3) (2022.7.1)\n",
+      "Requirement already satisfied: numpy>=1.16.5 in /home/kyle/miniconda3/envs/nyc_airbnb_dev/lib/python3.9/site-packages (from pandas==1.2.3) (1.23.5)\n",
+      "Requirement already satisfied: six>=1.5 in /home/kyle/miniconda3/envs/nyc_airbnb_dev/lib/python3.9/site-packages (from python-dateutil>=2.7.3->pandas==1.2.3) (1.16.0)\n"
      ]
     }
    ],
@@ -90,7 +152,7 @@
   },
   {
    "cell_type": "markdown",
-   "id": "54c4f98a",
+   "id": "00354a8b",
    "metadata": {},
    "source": [
     "1. Fetch the artifact we just created (sample.csv) from W&B and read it with pandas:"
@@ -98,64 +160,22 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 7,
-   "id": "eed98c34",
+   "execution_count": 2,
+   "id": "f3c3ceae",
    "metadata": {},
    "outputs": [
     {
-     "data": {
-      "text/html": [
-       "Finishing last run (ID:v3d3q3rf) before initializing another..."
-      ],
-      "text/plain": [
-       "<IPython.core.display.HTML object>"
-      ]
-     },
-     "metadata": {},
-     "output_type": "display_data"
-    },
-    {
-     "data": {
-      "text/html": [
-       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
-      ],
-      "text/plain": [
-       "<IPython.core.display.HTML object>"
-      ]
-     },
-     "metadata": {},
-     "output_type": "display_data"
-    },
-    {
-     "data": {
-      "application/vnd.jupyter.widget-view+json": {
-       "model_id": "a37b04273ead4a168e477c0394de3fcf",
-       "version_major": 2,
-       "version_minor": 0
-      },
-      "text/plain": [
-       "VBox(children=(Label(value='0.043 MB of 0.043 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
-      ]
-     },
-     "metadata": {},
-     "output_type": "display_data"
-    },
-    {
-     "data": {
-      "text/html": [
-       " View run <strong style=\"color:#cdcd00\">visionary-lake-13</strong> at: <a href=\"https://wandb.ai/annyang1963/nyc_airbnb/runs/v3d3q3rf\" target=\"_blank\">https://wandb.ai/annyang1963/nyc_airbnb/runs/v3d3q3rf</a><br/>Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)"
-      ],
-      "text/plain": [
-       "<IPython.core.display.HTML object>"
-      ]
-     },
-     "metadata": {},
-     "output_type": "display_data"
+     "name": "stderr",
+     "output_type": "stream",
+     "text": [
+      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkylenelsen\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
+     ]
     },
     {
      "data": {
       "text/html": [
-       "Find logs at: <code>./wandb/run-20230412_183324-v3d3q3rf/logs</code>"
+       "wandb version 0.16.2 is available!  To upgrade, please run:\n",
+       " $ pip install wandb --upgrade"
       ],
       "text/plain": [
        "<IPython.core.display.HTML object>"
@@ -167,7 +187,7 @@
     {
      "data": {
       "text/html": [
-       "Successfully finished last run (ID:v3d3q3rf). Initializing new run:<br/>"
+       "Tracking run with wandb version 0.13.9"
       ],
       "text/plain": [
        "<IPython.core.display.HTML object>"
@@ -176,25 +196,10 @@
      "metadata": {},
      "output_type": "display_data"
     },
-    {
-     "data": {
-      "application/vnd.jupyter.widget-view+json": {
-       "model_id": "110e5eb88c9f4f9bada3b547c8cd62eb",
-       "version_major": 2,
-       "version_minor": 0
-      },
-      "text/plain": [
-       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016671707533333326, max=1.0…"
-      ]
-     },
-     "metadata": {},
-     "output_type": "display_data"
-    },
     {
      "data": {
       "text/html": [
-       "wandb version 0.14.2 is available!  To upgrade, please run:\n",
-       " $ pip install wandb --upgrade"
+       "Run data is saved locally in <code>/mnt/f/WSL/Project-Build-an-ML-Pipeline-Starter/src/eda/wandb/run-20240202_114120-uxs7o0gw</code>"
       ],
       "text/plain": [
        "<IPython.core.display.HTML object>"
@@ -206,7 +211,7 @@
     {
      "data": {
       "text/html": [
-       "Tracking run with wandb version 0.13.9"
+       "Syncing run <strong><a href=\"https://wandb.ai/kylenelsen/nyc_airbnb/runs/uxs7o0gw\" target=\"_blank\">lunar-monkey-5</a></strong> to <a href=\"https://wandb.ai/kylenelsen/nyc_airbnb\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
       ],
       "text/plain": [
        "<IPython.core.display.HTML object>"
@@ -218,7 +223,7 @@
     {
      "data": {
       "text/html": [
-       "Run data is saved locally in <code>/Users/hyang/Huimin project/Project-Build-an-ML-Pipeline-Starter/src/eda/wandb/run-20230412_183404-cl95qrqk</code>"
+       " View project at <a href=\"https://wandb.ai/kylenelsen/nyc_airbnb\" target=\"_blank\">https://wandb.ai/kylenelsen/nyc_airbnb</a>"
       ],
       "text/plain": [
        "<IPython.core.display.HTML object>"
@@ -230,7 +235,7 @@
     {
      "data": {
       "text/html": [
-       "Syncing run <strong><a href=\"https://wandb.ai/annyang1963/nyc_airbnb/runs/cl95qrqk\" target=\"_blank\">summer-galaxy-14</a></strong> to <a href=\"https://wandb.ai/annyang1963/nyc_airbnb\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
+       " View run at <a href=\"https://wandb.ai/kylenelsen/nyc_airbnb/runs/uxs7o0gw\" target=\"_blank\">https://wandb.ai/kylenelsen/nyc_airbnb/runs/uxs7o0gw</a>"
       ],
       "text/plain": [
        "<IPython.core.display.HTML object>"
@@ -240,28 +245,23 @@
      "output_type": "display_data"
     },
     {
-     "data": {
-      "text/html": [
-       " View project at <a href=\"https://wandb.ai/annyang1963/nyc_airbnb\" target=\"_blank\">https://wandb.ai/annyang1963/nyc_airbnb</a>"
-      ],
-      "text/plain": [
-       "<IPython.core.display.HTML object>"
-      ]
-     },
-     "metadata": {},
-     "output_type": "display_data"
+     "name": "stdout",
+     "output_type": "stream",
+     "text": [
+      "Error in callback <bound method _WandbInit._pause_backend of <wandb.sdk.wandb_init._WandbInit object at 0x7f8e20106160>> (for post_run_cell), with arguments args (<ExecutionResult object at 7f8e2024fc10, execution_count=2 error_before_exec=None error_in_exec=None info=<ExecutionInfo object at 7f8e2024ffd0, raw_cell=\"import wandb\n",
+      "import pandas as pd\n",
+      "# Note that we us..\" store_history=True silent=False shell_futures=True cell_id=8b2a77ff-e450-40e3-8282-0581fd2d16fe> result=None>,),kwargs {}:\n"
+     ]
     },
     {
-     "data": {
-      "text/html": [
-       " View run at <a href=\"https://wandb.ai/annyang1963/nyc_airbnb/runs/cl95qrqk\" target=\"_blank\">https://wandb.ai/annyang1963/nyc_airbnb/runs/cl95qrqk</a>"
-      ],
-      "text/plain": [
-       "<IPython.core.display.HTML object>"
-      ]
-     },
-     "metadata": {},
-     "output_type": "display_data"
+     "ename": "TypeError",
+     "evalue": "_pause_backend() takes 1 positional argument but 2 were given",
+     "output_type": "error",
+     "traceback": [
+      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
+      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
+      "\u001b[0;31mTypeError\u001b[0m: _pause_backend() takes 1 positional argument but 2 were given"
+     ]
     }
    ],
    "source": [
@@ -275,7 +275,7 @@
   },
   {
    "cell_type": "markdown",
-   "id": "472b8c0d",
+   "id": "2ca191ec",
    "metadata": {},
    "source": [
     "2. Explore the data in df"
@@ -283,10 +283,27 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 8,
-   "id": "e7902159",
+   "execution_count": 3,
+   "id": "a847d472",
    "metadata": {},
    "outputs": [
+    {
+     "name": "stdout",
+     "output_type": "stream",
+     "text": [
+      "Error in callback <bound method _WandbInit._resume_backend of <wandb.sdk.wandb_init._WandbInit object at 0x7f8e20106160>> (for pre_run_cell), with arguments args (<ExecutionInfo object at 7f8de82c92b0, raw_cell=\"df.info()\" store_history=True silent=False shell_futures=True cell_id=d180e26c-5bec-4c9b-bdbe-5b2ebe35874c>,),kwargs {}:\n"
+     ]
+    },
+    {
+     "ename": "TypeError",
+     "evalue": "_resume_backend() takes 1 positional argument but 2 were given",
+     "output_type": "error",
+     "traceback": [
+      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
+      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
+      "\u001b[0;31mTypeError\u001b[0m: _resume_backend() takes 1 positional argument but 2 were given"
+     ]
+    },
     {
      "name": "stdout",
      "output_type": "stream",
@@ -313,7 +330,18 @@
       " 14  calculated_host_listings_count  20000 non-null  int64  \n",
       " 15  availability_365                20000 non-null  int64  \n",
       "dtypes: float64(3), int64(7), object(6)\n",
-      "memory usage: 2.4+ MB\n"
+      "memory usage: 2.4+ MB\n",
+      "Error in callback <bound method _WandbInit._pause_backend of <wandb.sdk.wandb_init._WandbInit object at 0x7f8e20106160>> (for post_run_cell), with arguments args (<ExecutionResult object at 7f8de82c9100, execution_count=3 error_before_exec=None error_in_exec=None info=<ExecutionInfo object at 7f8de82c92b0, raw_cell=\"df.info()\" store_history=True silent=False shell_futures=True cell_id=d180e26c-5bec-4c9b-bdbe-5b2ebe35874c> result=None>,),kwargs {}:\n"
+     ]
+    },
+    {
+     "ename": "TypeError",
+     "evalue": "_pause_backend() takes 1 positional argument but 2 were given",
+     "output_type": "error",
+     "traceback": [
+      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
+      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
+      "\u001b[0;31mTypeError\u001b[0m: _pause_backend() takes 1 positional argument but 2 were given"
      ]
     }
    ],
@@ -323,10 +351,27 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 9,
-   "id": "ef6f1457",
+   "execution_count": 4,
+   "id": "43216c74",
    "metadata": {},
    "outputs": [
+    {
+     "name": "stdout",
+     "output_type": "stream",
+     "text": [
+      "Error in callback <bound method _WandbInit._resume_backend of <wandb.sdk.wandb_init._WandbInit object at 0x7f8e20106160>> (for pre_run_cell), with arguments args (<ExecutionInfo object at 7f8de82c4850, raw_cell=\"df.describe()\" store_history=True silent=False shell_futures=True cell_id=cd8b82a1-a810-4a81-885e-77a886b39b02>,),kwargs {}:\n"
+     ]
+    },
+    {
+     "ename": "TypeError",
+     "evalue": "_resume_backend() takes 1 positional argument but 2 were given",
+     "output_type": "error",
+     "traceback": [
+      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
+      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
+      "\u001b[0;31mTypeError\u001b[0m: _resume_backend() takes 1 positional argument but 2 were given"
+     ]
+    },
     {
      "data": {
       "text/html": [
@@ -501,9 +546,54 @@
        "max                        327.000000        365.000000  "
       ]
      },
-     "execution_count": 9,
+     "execution_count": 4,
      "metadata": {},
      "output_type": "execute_result"
+    },
+    {
+     "name": "stdout",
+     "output_type": "stream",
+     "text": [
+      "Error in callback <bound method _WandbInit._pause_backend of <wandb.sdk.wandb_init._WandbInit object at 0x7f8e20106160>> (for post_run_cell), with arguments args (<ExecutionResult object at 7f8e2024f340, execution_count=4 error_before_exec=None error_in_exec=None info=<ExecutionInfo object at 7f8de82c4850, raw_cell=\"df.describe()\" store_history=True silent=False shell_futures=True cell_id=cd8b82a1-a810-4a81-885e-77a886b39b02> result=                 id       host_id      latitude     longitude         price  \\\n",
+      "count  2.000000e+04  2.000000e+04  20000.000000  20000.000000  20000.000000   \n",
+      "mean   1.892380e+07  6.746034e+07     40.728455    -73.952125    153.269050   \n",
+      "std    1.101223e+07  7.857936e+07      0.054755      0.046559    243.325609   \n",
+      "min    2.539000e+03  2.571000e+03     40.508730    -74.239140      0.000000   \n",
+      "25%    9.393540e+06  7.853718e+06     40.689420    -73.983030     69.000000   \n",
+      "50%    1.952117e+07  3.111431e+07     40.722730    -73.955640    105.000000   \n",
+      "75%    2.912936e+07  1.068426e+08     40.762990    -73.936380    175.000000   \n",
+      "max    3.648561e+07  2.742733e+08     40.913060    -73.717950  10000.000000   \n",
+      "\n",
+      "       minimum_nights  number_of_reviews  reviews_per_month  \\\n",
+      "count    20000.000000       20000.000000       15877.000000   \n",
+      "mean         6.992100          23.274100           1.377446   \n",
+      "std         21.645449          44.927793           1.683006   \n",
+      "min          1.000000           0.000000           0.010000   \n",
+      "25%          1.000000           1.000000           0.190000   \n",
+      "50%          2.000000           5.000000           0.720000   \n",
+      "75%          5.000000          23.000000           2.010000   \n",
+      "max       1250.000000         607.000000          27.950000   \n",
+      "\n",
+      "       calculated_host_listings_count  availability_365  \n",
+      "count                    20000.000000      20000.000000  \n",
+      "mean                         6.955450        112.901200  \n",
+      "std                         32.433831        131.762226  \n",
+      "min                          1.000000          0.000000  \n",
+      "25%                          1.000000          0.000000  \n",
+      "50%                          1.000000         44.000000  \n",
+      "75%                          2.000000        229.000000  \n",
+      "max                        327.000000        365.000000  >,),kwargs {}:\n"
+     ]
+    },
+    {
+     "ename": "TypeError",
+     "evalue": "_pause_backend() takes 1 positional argument but 2 were given",
+     "output_type": "error",
+     "traceback": [
+      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
+      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
+      "\u001b[0;31mTypeError\u001b[0m: _pause_backend() takes 1 positional argument but 2 were given"
+     ]
     }
    ],
    "source": [
@@ -512,10 +602,27 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 10,
-   "id": "13548f59",
+   "execution_count": 5,
+   "id": "31b32022",
    "metadata": {},
    "outputs": [
+    {
+     "name": "stdout",
+     "output_type": "stream",
+     "text": [
+      "Error in callback <bound method _WandbInit._resume_backend of <wandb.sdk.wandb_init._WandbInit object at 0x7f8e20106160>> (for pre_run_cell), with arguments args (<ExecutionInfo object at 7f8de8316400, raw_cell=\"df.head()\" store_history=True silent=False shell_futures=True cell_id=95064fee-c9b9-4e66-ab0e-b783b2292846>,),kwargs {}:\n"
+     ]
+    },
+    {
+     "ename": "TypeError",
+     "evalue": "_resume_backend() takes 1 positional argument but 2 were given",
+     "output_type": "error",
+     "traceback": [
+      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
+      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
+      "\u001b[0;31mTypeError\u001b[0m: _resume_backend() takes 1 positional argument but 2 were given"
+     ]
+    },
     {
      "data": {
       "text/html": [
@@ -685,9 +792,52 @@
        "4               0.52                               2                 8  "
       ]
      },
-     "execution_count": 10,
+     "execution_count": 5,
      "metadata": {},
      "output_type": "execute_result"
+    },
+    {
+     "name": "stdout",
+     "output_type": "stream",
+     "text": [
+      "Error in callback <bound method _WandbInit._pause_backend of <wandb.sdk.wandb_init._WandbInit object at 0x7f8e20106160>> (for post_run_cell), with arguments args (<ExecutionResult object at 7f8de8316e80, execution_count=5 error_before_exec=None error_in_exec=None info=<ExecutionInfo object at 7f8de8316400, raw_cell=\"df.head()\" store_history=True silent=False shell_futures=True cell_id=95064fee-c9b9-4e66-ab0e-b783b2292846> result=         id                                               name    host_id  \\\n",
+      "0   9138664                Private Lg Room 15 min to Manhattan   47594947   \n",
+      "1  31444015  TIME SQUARE CHARMING ONE BED IN HELL'S KITCHEN...    8523790   \n",
+      "2   8741020  Voted #1 Location Quintessential 1BR W Village...   45854238   \n",
+      "3  34602077  Spacious 1 bedroom apartment 15min from Manhattan  261055465   \n",
+      "4  23203149   Big beautiful bedroom in huge Bushwick apartment     143460   \n",
+      "\n",
+      "  host_name neighbourhood_group   neighbourhood  latitude  longitude  \\\n",
+      "0      Iris              Queens       Sunnyside  40.74271  -73.92493   \n",
+      "1    Johlex           Manhattan  Hell's Kitchen  40.76682  -73.98878   \n",
+      "2      John           Manhattan    West Village  40.73631  -74.00611   \n",
+      "3     Regan              Queens         Astoria  40.76424  -73.92351   \n",
+      "4     Megan            Brooklyn        Bushwick  40.69839  -73.92044   \n",
+      "\n",
+      "         room_type  price  minimum_nights  number_of_reviews last_review  \\\n",
+      "0     Private room     74               2                  6  2019-05-26   \n",
+      "1  Entire home/apt    170               3                  0         NaN   \n",
+      "2  Entire home/apt    245               3                 51  2018-09-19   \n",
+      "3  Entire home/apt    125               3                  1  2019-05-24   \n",
+      "4     Private room     65               2                  8  2019-06-23   \n",
+      "\n",
+      "   reviews_per_month  calculated_host_listings_count  availability_365  \n",
+      "0               0.13                               1                 5  \n",
+      "1                NaN                               1               188  \n",
+      "2               1.12                               1                 0  \n",
+      "3               0.65                               1                13  \n",
+      "4               0.52                               2                 8  >,),kwargs {}:\n"
+     ]
+    },
+    {
+     "ename": "TypeError",
+     "evalue": "_pause_backend() takes 1 positional argument but 2 were given",
+     "output_type": "error",
+     "traceback": [
+      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
+      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
+      "\u001b[0;31mTypeError\u001b[0m: _pause_backend() takes 1 positional argument but 2 were given"
+     ]
     }
    ],
    "source": [
@@ -696,7 +846,7 @@
   },
   {
    "cell_type": "markdown",
-   "id": "ecfee692",
+   "id": "67ca6d96",
    "metadata": {},
    "source": [
     "3. What do you notice in the data? Look around and see what you can find.\n",
@@ -706,7 +856,7 @@
   },
   {
    "cell_type": "markdown",
-   "id": "b4bb415f",
+   "id": "94e39437",
    "metadata": {},
    "source": [
     "4. Fix some of the little problems we have found in the data with the following code:"
@@ -714,10 +864,51 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 11,
-   "id": "4d685317",
+   "execution_count": 6,
+   "id": "b70f9594",
    "metadata": {},
-   "outputs": [],
+   "outputs": [
+    {
+     "name": "stdout",
+     "output_type": "stream",
+     "text": [
+      "Error in callback <bound method _WandbInit._resume_backend of <wandb.sdk.wandb_init._WandbInit object at 0x7f8e20106160>> (for pre_run_cell), with arguments args (<ExecutionInfo object at 7f8de8316670, raw_cell=\"# Drop outliers\n",
+      "min_price = 10\n",
+      "max_price = 350\n",
+      "idx..\" store_history=True silent=False shell_futures=True cell_id=db4f1fa1-a7d7-4188-a18f-a77be552af0a>,),kwargs {}:\n"
+     ]
+    },
+    {
+     "ename": "TypeError",
+     "evalue": "_resume_backend() takes 1 positional argument but 2 were given",
+     "output_type": "error",
+     "traceback": [
+      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
+      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
+      "\u001b[0;31mTypeError\u001b[0m: _resume_backend() takes 1 positional argument but 2 were given"
+     ]
+    },
+    {
+     "name": "stdout",
+     "output_type": "stream",
+     "text": [
+      "Error in callback <bound method _WandbInit._pause_backend of <wandb.sdk.wandb_init._WandbInit object at 0x7f8e20106160>> (for post_run_cell), with arguments args (<ExecutionResult object at 7f8de8316130, execution_count=6 error_before_exec=None error_in_exec=None info=<ExecutionInfo object at 7f8de8316670, raw_cell=\"# Drop outliers\n",
+      "min_price = 10\n",
+      "max_price = 350\n",
+      "idx..\" store_history=True silent=False shell_futures=True cell_id=db4f1fa1-a7d7-4188-a18f-a77be552af0a> result=None>,),kwargs {}:\n"
+     ]
+    },
+    {
+     "ename": "TypeError",
+     "evalue": "_pause_backend() takes 1 positional argument but 2 were given",
+     "output_type": "error",
+     "traceback": [
+      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
+      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
+      "\u001b[0;31mTypeError\u001b[0m: _pause_backend() takes 1 positional argument but 2 were given"
+     ]
+    }
+   ],
    "source": [
     "# Drop outliers\n",
     "min_price = 10\n",
@@ -730,7 +921,7 @@
   },
   {
    "cell_type": "markdown",
-   "id": "17990df4",
+   "id": "99eb337c",
    "metadata": {},
    "source": [
     "Note how we did not impute missing values. We will do that in the inference pipeline, so we will be able to handle missing values also in production."
@@ -738,7 +929,7 @@
   },
   {
    "cell_type": "markdown",
-   "id": "09287f29",
+   "id": "f2da3199",
    "metadata": {},
    "source": [
     "5. Check with df.info() that all obvious problems have been solved"
@@ -746,10 +937,27 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 12,
-   "id": "b83b4970",
+   "execution_count": 7,
+   "id": "a93a1b18",
    "metadata": {},
    "outputs": [
+    {
+     "name": "stdout",
+     "output_type": "stream",
+     "text": [
+      "Error in callback <bound method _WandbInit._resume_backend of <wandb.sdk.wandb_init._WandbInit object at 0x7f8e20106160>> (for pre_run_cell), with arguments args (<ExecutionInfo object at 7f8e20106af0, raw_cell=\"df.info()\" store_history=True silent=False shell_futures=True cell_id=95944dbc-e6b9-4815-8180-3352cceec9c0>,),kwargs {}:\n"
+     ]
+    },
+    {
+     "ename": "TypeError",
+     "evalue": "_resume_backend() takes 1 positional argument but 2 were given",
+     "output_type": "error",
+     "traceback": [
+      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
+      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
+      "\u001b[0;31mTypeError\u001b[0m: _resume_backend() takes 1 positional argument but 2 were given"
+     ]
+    },
     {
      "name": "stdout",
      "output_type": "stream",
@@ -776,7 +984,18 @@
       " 14  calculated_host_listings_count  19001 non-null  int64         \n",
       " 15  availability_365                19001 non-null  int64         \n",
       "dtypes: datetime64[ns](1), float64(3), int64(7), object(5)\n",
-      "memory usage: 2.5+ MB\n"
+      "memory usage: 2.5+ MB\n",
+      "Error in callback <bound method _WandbInit._pause_backend of <wandb.sdk.wandb_init._WandbInit object at 0x7f8e20106160>> (for post_run_cell), with arguments args (<ExecutionResult object at 7f8e20106d90, execution_count=7 error_before_exec=None error_in_exec=None info=<ExecutionInfo object at 7f8e20106af0, raw_cell=\"df.info()\" store_history=True silent=False shell_futures=True cell_id=95944dbc-e6b9-4815-8180-3352cceec9c0> result=None>,),kwargs {}:\n"
+     ]
+    },
+    {
+     "ename": "TypeError",
+     "evalue": "_pause_backend() takes 1 positional argument but 2 were given",
+     "output_type": "error",
+     "traceback": [
+      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
+      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
+      "\u001b[0;31mTypeError\u001b[0m: _pause_backend() takes 1 positional argument but 2 were given"
      ]
     }
    ],
@@ -786,7 +1005,7 @@
   },
   {
    "cell_type": "markdown",
-   "id": "d727e697",
+   "id": "f9e608b8",
    "metadata": {},
    "source": [
     "6. Terminate the run by running `run.finish()`"
@@ -794,10 +1013,27 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 13,
-   "id": "48b38586",
+   "execution_count": 8,
+   "id": "c65ab333",
    "metadata": {},
    "outputs": [
+    {
+     "name": "stdout",
+     "output_type": "stream",
+     "text": [
+      "Error in callback <bound method _WandbInit._resume_backend of <wandb.sdk.wandb_init._WandbInit object at 0x7f8e20106160>> (for pre_run_cell), with arguments args (<ExecutionInfo object at 7f8de8324f40, raw_cell=\"run.finish()\" store_history=True silent=False shell_futures=True cell_id=8c32476d-d276-4c25-a7c1-a7e8cf1be8d0>,),kwargs {}:\n"
+     ]
+    },
+    {
+     "ename": "TypeError",
+     "evalue": "_resume_backend() takes 1 positional argument but 2 were given",
+     "output_type": "error",
+     "traceback": [
+      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
+      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
+      "\u001b[0;31mTypeError\u001b[0m: _resume_backend() takes 1 positional argument but 2 were given"
+     ]
+    },
     {
      "data": {
       "text/html": [
@@ -810,24 +1046,10 @@
      "metadata": {},
      "output_type": "display_data"
     },
-    {
-     "data": {
-      "application/vnd.jupyter.widget-view+json": {
-       "model_id": "0a45b778e49c4dfeb3cc16a0684831b0",
-       "version_major": 2,
-       "version_minor": 0
-      },
-      "text/plain": [
-       "VBox(children=(Label(value='0.043 MB of 0.057 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=0.748456…"
-      ]
-     },
-     "metadata": {},
-     "output_type": "display_data"
-    },
     {
      "data": {
       "text/html": [
-       " View run <strong style=\"color:#cdcd00\">summer-galaxy-14</strong> at: <a href=\"https://wandb.ai/annyang1963/nyc_airbnb/runs/cl95qrqk\" target=\"_blank\">https://wandb.ai/annyang1963/nyc_airbnb/runs/cl95qrqk</a><br/>Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
+       " View run <strong style=\"color:#cdcd00\">lunar-monkey-5</strong> at: <a href=\"https://wandb.ai/kylenelsen/nyc_airbnb/runs/uxs7o0gw\" target=\"_blank\">https://wandb.ai/kylenelsen/nyc_airbnb/runs/uxs7o0gw</a><br/>Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)"
       ],
       "text/plain": [
        "<IPython.core.display.HTML object>"
@@ -839,7 +1061,7 @@
     {
      "data": {
       "text/html": [
-       "Find logs at: <code>./wandb/run-20230412_183404-cl95qrqk/logs</code>"
+       "Find logs at: <code>./wandb/run-20240202_114120-uxs7o0gw/logs</code>"
       ],
       "text/plain": [
        "<IPython.core.display.HTML object>"
@@ -855,7 +1077,7 @@
   },
   {
    "cell_type": "markdown",
-   "id": "13f82b21",
+   "id": "3693bfb7",
    "metadata": {},
    "source": [
     "7. Save the notebook."
diff --git a/src/train_random_forest/MLproject b/src/train_random_forest/MLproject
index 3d4ae2f..b455a2b 100644
--- a/src/train_random_forest/MLproject
+++ b/src/train_random_forest/MLproject
@@ -1,46 +1,46 @@
-name: download_file
-conda_env: conda.yml
-
-entry_points:
-  main:
-    parameters:
-
-      trainval_artifact:
-        description: Train dataset
-        type: string
-
-      val_size:
-        description: Size of the validation split. Fraction of the dataset, or number of items
-        type: string
-
-      random_seed:
-        description: Seed for the random number generator. Use this for reproducibility
-        type: string
-        default: 42
-
-      stratify_by:
-        description: Column to use for stratification (if any)
-        type: string
-        default: 'none'
-
-      rf_config:
-        description: Random forest configuration. A path to a JSON file with the configuration that will
-                     be passed to the scikit-learn constructor for RandomForestRegressor.
-        type: string
-
-      max_tfidf_features:
-        description: Maximum number of words to consider for the TFIDF
-        type: string
-
-      output_artifact:
-        description: Name for the output artifact
-        type: string
-
-    command: >-
-      python run.py --trainval_artifact {trainval_artifact} \
-                    --val_size {val_size} \
-                    --random_seed {random_seed} \
-                    --stratify_by {stratify_by} \
-                    --rf_config {rf_config} \
-                    --max_tfidf_features {max_tfidf_features} \
-                    --output_artifact {output_artifact}
+name: download_file
+conda_env: conda.yml
+
+entry_points:
+  main:
+    parameters:
+
+      trainval_artifact:
+        description: Train dataset
+        type: string
+
+      val_size:
+        description: Size of the validation split. Fraction of the dataset, or number of items
+        type: string
+
+      random_seed:
+        description: Seed for the random number generator. Use this for reproducibility
+        type: string
+        default: 42
+
+      stratify_by:
+        description: Column to use for stratification (if any)
+        type: string
+        default: 'none'
+
+      rf_config:
+        description: Random forest configuration. A path to a JSON file with the configuration that will
+                     be passed to the scikit-learn constructor for RandomForestRegressor.
+        type: string
+
+      max_tfidf_features:
+        description: Maximum number of words to consider for the TFIDF
+        type: string
+
+      output_artifact:
+        description: Name for the output artifact
+        type: string
+
+    command: >-
+      python run.py --trainval_artifact {trainval_artifact} \
+                    --val_size {val_size} \
+                    --random_seed {random_seed} \
+                    --stratify_by {stratify_by} \
+                    --rf_config {rf_config} \
+                    --max_tfidf_features {max_tfidf_features} \
+                    --output_artifact {output_artifact}
diff --git a/src/train_random_forest/conda.yml b/src/train_random_forest/conda.yml
index 2335152..b0c98d0 100644
--- a/src/train_random_forest/conda.yml
+++ b/src/train_random_forest/conda.yml
@@ -1,13 +1,13 @@
-name: basic_cleaning
-channels:
-  - conda-forge
-  - defaults
-dependencies:
-  - pandas=1.1.4
-  - pip=20.3.3
-  - mlflow=2.1.1
-  - scikit-learn=0.24.1
-  - matplotlib=3.6.2
-  - pillow=8.1.2
-  - pip:
-      - wandb==0.13.9
+name: basic_cleaning
+channels:
+  - conda-forge
+  - defaults
+dependencies:
+  - pandas=1.1.4
+  - pip=20.3.3
+  - mlflow=2.1.1
+  - scikit-learn=0.24.1
+  - matplotlib=3.6.2
+  - pillow=8.1.2
+  - pip:
+      - wandb==0.13.9
diff --git a/src/train_random_forest/feature_engineering.py b/src/train_random_forest/feature_engineering.py
index d4a1c73..8686be1 100644
--- a/src/train_random_forest/feature_engineering.py
+++ b/src/train_random_forest/feature_engineering.py
@@ -1,11 +1,11 @@
-import pandas as pd
-import numpy as np
-
-
-def delta_date_feature(dates):
-    """
-    Given a 2d array containing dates (in any format recognized by pd.to_datetime), it returns the delta in days
-    between each date and the most recent date in its column
-    """
-    date_sanitized = pd.DataFrame(dates).apply(pd.to_datetime)
-    return date_sanitized.apply(lambda d: (d.max() -d).dt.days, axis=0).to_numpy()
+import pandas as pd
+import numpy as np
+
+
+def delta_date_feature(dates):
+    """
+    Given a 2d array containing dates (in any format recognized by pd.to_datetime), it returns the delta in days
+    between each date and the most recent date in its column
+    """
+    date_sanitized = pd.DataFrame(dates).apply(pd.to_datetime)
+    return date_sanitized.apply(lambda d: (d.max() -d).dt.days, axis=0).to_numpy()
diff --git a/src/train_random_forest/run.py b/src/train_random_forest/run.py
index cc35d9b..bcdb270 100644
--- a/src/train_random_forest/run.py
+++ b/src/train_random_forest/run.py
@@ -1,293 +1,300 @@
-#!/usr/bin/env python
-"""
-This script trains a Random Forest
-"""
-import argparse
-import logging
-import os
-import shutil
-import matplotlib.pyplot as plt
-
-import mlflow
-import json
-
-import pandas as pd
-import numpy as np
-from sklearn.compose import ColumnTransformer
-from sklearn.feature_extraction.text import TfidfVectorizer
-from sklearn.impute import SimpleImputer
-from sklearn.model_selection import train_test_split
-from sklearn.preprocessing import OrdinalEncoder, OneHotEncoder, FunctionTransformer
-
-import wandb
-from sklearn.ensemble import RandomForestRegressor
-from sklearn.metrics import mean_absolute_error
-from sklearn.pipeline import Pipeline, make_pipeline
-
-
-def delta_date_feature(dates):
-    """
-    Given a 2d array containing dates (in any format recognized by pd.to_datetime), it returns the delta in days
-    between each date and the most recent date in its column
-    """
-    date_sanitized = pd.DataFrame(dates).apply(pd.to_datetime)
-    return date_sanitized.apply(lambda d: (d.max() -d).dt.days, axis=0).to_numpy()
-
-
-logging.basicConfig(level=logging.INFO, format="%(asctime)-15s %(message)s")
-logger = logging.getLogger()
-
-
-def go(args):
-
-    run = wandb.init(job_type="train_random_forest")
-    run.config.update(args)
-
-    # Get the Random Forest configuration and update W&B
-    with open(args.rf_config) as fp:
-        rf_config = json.load(fp)
-    run.config.update(rf_config)
-
-    # Fix the random seed for the Random Forest, so we get reproducible results
-    rf_config['random_state'] = args.random_seed
-
-    # Use run.use_artifact(...).file() to get the train and validation artifact
-    # and save the returned path in train_local_pat
-    trainval_local_path = run.use_artifact(args.trainval_artifact).file()
-   
-    X = pd.read_csv(trainval_local_path)
-    y = X.pop("price")  # this removes the column "price" from X and puts it into y
-
-    logger.info(f"Minimum price: {y.min()}, Maximum price: {y.max()}")
-
-    X_train, X_val, y_train, y_val = train_test_split(
-        X, y, test_size=args.val_size, stratify=X[args.stratify_by], random_state=args.random_seed
-    )
-
-    logger.info("Preparing sklearn pipeline")
-
-    sk_pipe, processed_features = get_inference_pipeline(rf_config, args.max_tfidf_features)
-
-    # Then fit it to the X_train, y_train data
-    logger.info("Fitting")
-
-    ######################################
-    # Fit the pipeline sk_pipe by calling the .fit method on X_train and y_train
-    # YOUR CODE HERE
-    ######################################
-
-    # Compute r2 and MAE
-    logger.info("Scoring")
-    r_squared = sk_pipe.score(X_val, y_val)
-
-    y_pred = sk_pipe.predict(X_val)
-    mae = mean_absolute_error(y_val, y_pred)
-
-    logger.info(f"Score: {r_squared}")
-    logger.info(f"MAE: {mae}")
-
-    logger.info("Exporting model")
-
-    # Save model package in the MLFlow sklearn format
-    if os.path.exists("random_forest_dir"):
-        shutil.rmtree("random_forest_dir")
-
-    ######################################
-    # Save the sk_pipe pipeline as a mlflow.sklearn model in the directory "random_forest_dir"
-    # HINT: use mlflow.sklearn.save_model
-    signature = mlflow.models.infer_signature(X_val, y_pred)
-    mlflow.sklearn.save_model(
-        # YOUR CODE HERE
-        signature = signature,
-        input_example = X_train.iloc[:5]
-    )
-    ######################################
-
-
-    # Upload the model we just exported to W&B
-    artifact = wandb.Artifact(
-        args.output_artifact,
-        type = 'model_export',
-        description = 'Trained ranfom forest artifact',
-        metadata = rf_config
-    )
-    artifact.add_dir('random_forest_dir')
-    run.log_artifact(artifact)
-
-    # Plot feature importance
-    fig_feat_imp = plot_feature_importance(sk_pipe, processed_features)
-
-    ######################################
-    # Here we save variable r_squared under the "r2" key
-    run.summary['r2'] = r_squared
-    # Now save the variable mae under the key "mae".
-    # YOUR CODE HERE
-    ######################################
-
-    # Upload to W&B the feture importance visualization
-    run.log(
-        {
-          "feature_importance": wandb.Image(fig_feat_imp),
-        }
-    )
-
-
-def plot_feature_importance(pipe, feat_names):
-    # We collect the feature importance for all non-nlp features first
-    feat_imp = pipe["random_forest"].feature_importances_[: len(feat_names)-1]
-    # For the NLP feature we sum across all the TF-IDF dimensions into a global
-    # NLP importance
-    nlp_importance = sum(pipe["random_forest"].feature_importances_[len(feat_names) - 1:])
-    feat_imp = np.append(feat_imp, nlp_importance)
-    fig_feat_imp, sub_feat_imp = plt.subplots(figsize=(10, 10))
-    # idx = np.argsort(feat_imp)[::-1]
-    sub_feat_imp.bar(range(feat_imp.shape[0]), feat_imp, color="r", align="center")
-    _ = sub_feat_imp.set_xticks(range(feat_imp.shape[0]))
-    _ = sub_feat_imp.set_xticklabels(np.array(feat_names), rotation=90)
-    fig_feat_imp.tight_layout()
-    return fig_feat_imp
-
-
-def get_inference_pipeline(rf_config, max_tfidf_features):
-    # Let's handle the categorical features first
-    # Ordinal categorical are categorical values for which the order is meaningful, for example
-    # for room type: 'Entire home/apt' > 'Private room' > 'Shared room'
-    ordinal_categorical = ["room_type"]
-    non_ordinal_categorical = ["neighbourhood_group"]
-    # NOTE: we do not need to impute room_type because the type of the room
-    # is mandatory on the websites, so missing values are not possible in production
-    # (nor during training). That is not true for neighbourhood_group
-    ordinal_categorical_preproc = OrdinalEncoder()
-
-    ######################################
-    # Build a pipeline with two steps:
-    # 1 - A SimpleImputer(strategy="most_frequent") to impute missing values
-    # 2 - A OneHotEncoder() step to encode the variable
-    non_ordinal_categorical_preproc = make_pipeline(
-        # YOUR CODE HERE
-    )
-    ######################################
-
-    # Let's impute the numerical columns to make sure we can handle missing values
-    # (note that we do not scale because the RF algorithm does not need that)
-    zero_imputed = [
-        "minimum_nights",
-        "number_of_reviews",
-        "reviews_per_month",
-        "calculated_host_listings_count",
-        "availability_365",
-        "longitude",
-        "latitude"
-    ]
-    zero_imputer = SimpleImputer(strategy="constant", fill_value=0)
-
-    # A MINIMAL FEATURE ENGINEERING step:
-    # we create a feature that represents the number of days passed since the last review
-    # First we impute the missing review date with an old date (because there hasn't been
-    # a review for a long time), and then we create a new feature from it,
-    date_imputer = make_pipeline(
-        SimpleImputer(strategy='constant', fill_value='2010-01-01'),
-        FunctionTransformer(delta_date_feature, check_inverse=False, validate=False)
-    )
-
-    # Some minimal NLP for the "name" column
-    reshape_to_1d = FunctionTransformer(np.reshape, kw_args={"newshape": -1})
-    name_tfidf = make_pipeline(
-        SimpleImputer(strategy="constant", fill_value=""),
-        reshape_to_1d,
-        TfidfVectorizer(
-            binary=False,
-            max_features=max_tfidf_features,
-            stop_words='english'
-        ),
-    )
-
-    # Let's put everything together
-    preprocessor = ColumnTransformer(
-        transformers=[
-            ("ordinal_cat", ordinal_categorical_preproc, ordinal_categorical),
-            ("non_ordinal_cat", non_ordinal_categorical_preproc, non_ordinal_categorical),
-            ("impute_zero", zero_imputer, zero_imputed),
-            ("transform_date", date_imputer, ["last_review"]),
-            ("transform_name", name_tfidf, ["name"])
-        ],
-        remainder="drop",  # This drops the columns that we do not transform
-    )
-
-    processed_features = ordinal_categorical + non_ordinal_categorical + zero_imputed + ["last_review", "name"]
-
-    # Create random forest
-    random_forest = RandomForestRegressor(**rf_config)
-
-    ######################################
-    # Create the inference pipeline. The pipeline must have 2 steps: 
-    # 1 - a step called "preprocessor" applying the ColumnTransformer instance that we saved in the `preprocessor` variable
-    # 2 - a step called "random_forest" with the random forest instance that we just saved in the `random_forest` variable.
-    # HINT: Use the explicit Pipeline constructor so you can assign the names to the steps, do not use make_pipeline
-
-    sk_pipe = Pipeline(
-        steps =[
-        # YOUR CODE HERE
-        ]
-    )
-
-    return sk_pipe, processed_features
-    ######################################
-
-
-if __name__ == "__main__":
-
-    parser = argparse.ArgumentParser(description="Basic cleaning of dataset")
-
-    parser.add_argument(
-        "--trainval_artifact",
-        type=str,
-        help="Artifact containing the training dataset. It will be split into train and validation"
-    )
-
-    parser.add_argument(
-        "--val_size",
-        type=float,
-        help="Size of the validation split. Fraction of the dataset, or number of items",
-    )
-
-    parser.add_argument(
-        "--random_seed",
-        type=int,
-        help="Seed for random number generator",
-        default=42,
-        required=False,
-    )
-
-    parser.add_argument(
-        "--stratify_by",
-        type=str,
-        help="Column to use for stratification",
-        default="none",
-        required=False,
-    )
-
-    parser.add_argument(
-        "--rf_config",
-        help="Random forest configuration. A JSON dict that will be passed to the "
-        "scikit-learn constructor for RandomForestRegressor.",
-        default="{}",
-    )
-
-    parser.add_argument(
-        "--max_tfidf_features",
-        help="Maximum number of words to consider for the TFIDF",
-        default=10,
-        type=int
-    )
-
-    parser.add_argument(
-        "--output_artifact",
-        type=str,
-        help="Name for the output serialized model",
-        required=True,
-    )
-
-    args = parser.parse_args()
-
-    go(args)
+#!/usr/bin/env python
+"""
+This script trains a Random Forest
+"""
+import argparse
+import logging
+import os
+import shutil
+import matplotlib.pyplot as plt
+
+import mlflow
+import json
+
+import pandas as pd
+import numpy as np
+from sklearn.compose import ColumnTransformer
+from sklearn.feature_extraction.text import TfidfVectorizer
+from sklearn.impute import SimpleImputer
+from sklearn.model_selection import train_test_split
+from sklearn.preprocessing import OrdinalEncoder, OneHotEncoder, FunctionTransformer
+
+import wandb
+from sklearn.ensemble import RandomForestRegressor
+from sklearn.metrics import mean_absolute_error
+from sklearn.pipeline import Pipeline, make_pipeline
+
+
+def delta_date_feature(dates):
+    """
+    Given a 2d array containing dates (in any format recognized by pd.to_datetime), it returns the delta in days
+    between each date and the most recent date in its column
+    """
+    date_sanitized = pd.DataFrame(dates).apply(pd.to_datetime)
+    return date_sanitized.apply(lambda d: (d.max() -d).dt.days, axis=0).to_numpy()
+
+
+logging.basicConfig(level=logging.INFO, format="%(asctime)-15s %(message)s")
+logger = logging.getLogger()
+
+
+def go(args):
+
+    run = wandb.init(job_type="train_random_forest")
+    run.config.update(args)
+
+    # Get the Random Forest configuration and update W&B
+    with open(args.rf_config) as fp:
+        rf_config = json.load(fp)
+    run.config.update(rf_config)
+
+    # Fix the random seed for the Random Forest, so we get reproducible results
+    rf_config['random_state'] = args.random_seed
+
+    # Use run.use_artifact(...).file() to get the train and validation artifact
+    # and save the returned path in train_local_pat
+    trainval_local_path = run.use_artifact(args.trainval_artifact).file()
+   
+    X = pd.read_csv(trainval_local_path)
+    y = X.pop("price")  # this removes the column "price" from X and puts it into y
+
+    logger.info(f"Minimum price: {y.min()}, Maximum price: {y.max()}")
+
+    X_train, X_val, y_train, y_val = train_test_split(
+        X, y, test_size=args.val_size, stratify=X[args.stratify_by], random_state=args.random_seed
+    )
+
+    logger.info("Preparing sklearn pipeline")
+
+    sk_pipe, processed_features = get_inference_pipeline(rf_config, args.max_tfidf_features)
+
+    # Then fit it to the X_train, y_train data
+    logger.info("Fitting")
+
+    ######################################
+    # Fit the pipeline sk_pipe by calling the .fit method on X_train and y_train
+    sk_pipe.fit(X_train, y_train)
+    ######################################
+
+    # Compute r2 and MAE
+    logger.info("Scoring")
+    r_squared = sk_pipe.score(X_val, y_val)
+
+    y_pred = sk_pipe.predict(X_val)
+    mae = mean_absolute_error(y_val, y_pred)
+
+    logger.info(f"Score: {r_squared}")
+    logger.info(f"MAE: {mae}")
+
+    logger.info("Exporting model")
+
+    # Save model package in the MLFlow sklearn format
+    if os.path.exists("random_forest_dir"):
+        shutil.rmtree("random_forest_dir")
+
+    ######################################
+    # Save the sk_pipe pipeline as a mlflow.sklearn model in the directory "random_forest_dir"
+    # HINT: use mlflow.sklearn.save_model
+    signature = mlflow.models.infer_signature(X_val, y_pred)
+    mlflow.sklearn.save_model(
+        # YOUR CODE HERE
+        sk_pipe,
+        path="random_forest_dir",
+        signature = signature,
+        input_example = X_train.iloc[:5]
+    )
+    ######################################
+
+
+    # Upload the model we just exported to W&B
+    artifact = wandb.Artifact(
+        args.output_artifact,
+        type = 'model_export',
+        description = 'Trained ranfom forest artifact',
+        metadata = rf_config
+    )
+    artifact.add_dir('random_forest_dir')
+    run.log_artifact(artifact)
+
+    # Plot feature importance
+    fig_feat_imp = plot_feature_importance(sk_pipe, processed_features)
+
+    ######################################
+    # Here we save variable r_squared under the "r2" key
+    run.summary['r2'] = r_squared
+    # Now save the variable mae under the key "mae".
+    # YOUR CODE HERE
+    run.summary['mae'] = mae
+    ######################################
+
+    # Upload to W&B the feture importance visualization
+    run.log(
+        {
+          "feature_importance": wandb.Image(fig_feat_imp),
+        }
+    )
+
+
+def plot_feature_importance(pipe, feat_names):
+    # We collect the feature importance for all non-nlp features first
+    feat_imp = pipe["random_forest"].feature_importances_[: len(feat_names)-1]
+    # For the NLP feature we sum across all the TF-IDF dimensions into a global
+    # NLP importance
+    nlp_importance = sum(pipe["random_forest"].feature_importances_[len(feat_names) - 1:])
+    feat_imp = np.append(feat_imp, nlp_importance)
+    fig_feat_imp, sub_feat_imp = plt.subplots(figsize=(10, 10))
+    # idx = np.argsort(feat_imp)[::-1]
+    sub_feat_imp.bar(range(feat_imp.shape[0]), feat_imp, color="r", align="center")
+    _ = sub_feat_imp.set_xticks(range(feat_imp.shape[0]))
+    _ = sub_feat_imp.set_xticklabels(np.array(feat_names), rotation=90)
+    fig_feat_imp.tight_layout()
+    return fig_feat_imp
+
+
+def get_inference_pipeline(rf_config, max_tfidf_features):
+    # Let's handle the categorical features first
+    # Ordinal categorical are categorical values for which the order is meaningful, for example
+    # for room type: 'Entire home/apt' > 'Private room' > 'Shared room'
+    ordinal_categorical = ["room_type"]
+    non_ordinal_categorical = ["neighbourhood_group"]
+    # NOTE: we do not need to impute room_type because the type of the room
+    # is mandatory on the websites, so missing values are not possible in production
+    # (nor during training). That is not true for neighbourhood_group
+    ordinal_categorical_preproc = OrdinalEncoder()
+
+    ######################################
+    # Build a pipeline with two steps:
+    # 1 - A SimpleImputer(strategy="most_frequent") to impute missing values
+    # 2 - A OneHotEncoder() step to encode the variable
+    non_ordinal_categorical_preproc = make_pipeline(
+        # YOUR CODE HERE
+        SimpleImputer(strategy="most_frequent"),
+        OneHotEncoder()
+    )
+    ######################################
+
+    # Let's impute the numerical columns to make sure we can handle missing values
+    # (note that we do not scale because the RF algorithm does not need that)
+    zero_imputed = [
+        "minimum_nights",
+        "number_of_reviews",
+        "reviews_per_month",
+        "calculated_host_listings_count",
+        "availability_365",
+        "longitude",
+        "latitude"
+    ]
+    zero_imputer = SimpleImputer(strategy="constant", fill_value=0)
+
+    # A MINIMAL FEATURE ENGINEERING step:
+    # we create a feature that represents the number of days passed since the last review
+    # First we impute the missing review date with an old date (because there hasn't been
+    # a review for a long time), and then we create a new feature from it,
+    date_imputer = make_pipeline(
+        SimpleImputer(strategy='constant', fill_value='2010-01-01'),
+        FunctionTransformer(delta_date_feature, check_inverse=False, validate=False)
+    )
+
+    # Some minimal NLP for the "name" column
+    reshape_to_1d = FunctionTransformer(np.reshape, kw_args={"newshape": -1})
+    name_tfidf = make_pipeline(
+        SimpleImputer(strategy="constant", fill_value=""),
+        reshape_to_1d,
+        TfidfVectorizer(
+            binary=False,
+            max_features=max_tfidf_features,
+            stop_words='english'
+        ),
+    )
+
+    # Let's put everything together
+    preprocessor = ColumnTransformer(
+        transformers=[
+            ("ordinal_cat", ordinal_categorical_preproc, ordinal_categorical),
+            ("non_ordinal_cat", non_ordinal_categorical_preproc, non_ordinal_categorical),
+            ("impute_zero", zero_imputer, zero_imputed),
+            ("transform_date", date_imputer, ["last_review"]),
+            ("transform_name", name_tfidf, ["name"])
+        ],
+        remainder="drop",  # This drops the columns that we do not transform
+    )
+
+    processed_features = ordinal_categorical + non_ordinal_categorical + zero_imputed + ["last_review", "name"]
+
+    # Create random forest
+    random_forest = RandomForestRegressor(**rf_config)
+
+    ######################################
+    # Create the inference pipeline. The pipeline must have 2 steps: 
+    # 1 - a step called "preprocessor" applying the ColumnTransformer instance that we saved in the `preprocessor` variable
+    # 2 - a step called "random_forest" with the random forest instance that we just saved in the `random_forest` variable.
+    # HINT: Use the explicit Pipeline constructor so you can assign the names to the steps, do not use make_pipeline
+
+    sk_pipe = Pipeline(
+        steps =[
+        # YOUR CODE HERE
+        ("preprocessor", preprocessor),
+        ("random_forest", random_forest) 
+        ]
+    )
+
+    return sk_pipe, processed_features
+    ######################################
+
+
+if __name__ == "__main__":
+
+    parser = argparse.ArgumentParser(description="Basic cleaning of dataset")
+
+    parser.add_argument(
+        "--trainval_artifact",
+        type=str,
+        help="Artifact containing the training dataset. It will be split into train and validation"
+    )
+
+    parser.add_argument(
+        "--val_size",
+        type=float,
+        help="Size of the validation split. Fraction of the dataset, or number of items",
+    )
+
+    parser.add_argument(
+        "--random_seed",
+        type=int,
+        help="Seed for random number generator",
+        default=42,
+        required=False,
+    )
+
+    parser.add_argument(
+        "--stratify_by",
+        type=str,
+        help="Column to use for stratification",
+        default="none",
+        required=False,
+    )
+
+    parser.add_argument(
+        "--rf_config",
+        help="Random forest configuration. A JSON dict that will be passed to the "
+        "scikit-learn constructor for RandomForestRegressor.",
+        default="{}",
+    )
+
+    parser.add_argument(
+        "--max_tfidf_features",
+        help="Maximum number of words to consider for the TFIDF",
+        default=10,
+        type=int
+    )
+
+    parser.add_argument(
+        "--output_artifact",
+        type=str,
+        help="Name for the output serialized model",
+        required=True,
+    )
+
+    args = parser.parse_args()
+
+    go(args)
